<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>keras_nlp.mappings.word_vectors API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>keras_nlp.mappings.word_vectors</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import abc
import struct
import logging
import numpy as np
from keras.utils.generic_utils import Progbar
from keras.layers import Embedding


class WordVectors:
    &#34;&#34;&#34;
    A class to define common methods and functionality to load word vectors
    from different formats.

    Examples
    --------
    &gt;&gt;&gt; import tempfile
    &gt;&gt;&gt; from keras_nlp import Glove, WordVectorizer
    &gt;&gt;&gt; vectors_file = tempfile.NamedTemporaryFile()
    &gt;&gt;&gt; vectors_file.write(b&#39;phasellus 0.1 -0.3 0.2\\n&#39;)  #doctest:+SKIP
    &gt;&gt;&gt; vectors_file.write(b&#39;fermentum 0.2 0.1 -0.1\\n&#39;)  #doctest:+SKIP
    &gt;&gt;&gt; vectors_file.seek(0)  #doctest:+SKIP
    &gt;&gt;&gt; texts = [&#39;Phasellus fermentum tellus eget libero sodales varius.&#39;,
    ... &#39;In vestibulum erat nec nulla porttitor.&#39;]
    &gt;&gt;&gt; word_vectorizer = WordVectorizer(oov_token=&#39;_UNK_&#39;, verbose=0)
    &gt;&gt;&gt; word_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; glove = Glove(word_vectorizer.token2id, word_vectorizer.oov_token)
    &gt;&gt;&gt; glove.load(vectors_file.name)

    Validate the loaded vectors.
    &gt;&gt;&gt; arr1 = glove[&#39;phasellus&#39;]
    &gt;&gt;&gt; arr2 = np.asarray([0.1, -0.3, 0.2])
    &gt;&gt;&gt; np.testing.assert_array_almost_equal(arr1, arr2, decimal=1)

    Embedding layer input dim should match vocabulary number of tokens.
    &gt;&gt;&gt; embedding_layer = glove.get_embedding_layer(input_length=7)
    &gt;&gt;&gt; assert embedding_layer.input_dim = word_vectorizer.num_tokens
    &#34;&#34;&#34;

    def __init__(self, vocab, oov_token=None):
        &#34;&#34;&#34;
        Parameters
        ----------
        vocab : dict
            A token to id mapping.

        oov_token : str or None, default None
            If the vocabulary has an out-of-vocabulary token, then this
            parameter should have the key of that token. A random vector is
            create with uniform distribution in range [-0.05, 0.05].
        &#34;&#34;&#34;
        self.vocab = vocab
        self.oov_token = oov_token
        self.vector_len = 0
        self.vectors = None

        self.logger = logging.getLogger(self.__class__.__name__)

    def _init_vectors(self):
        &#34;&#34;&#34;
        Initialize vectors array and insert a random vector in case of having
        an out-of-vocabulary token.
        &#34;&#34;&#34;
        self.vectors = np.zeros(shape=(len(self.vocab), self.vector_len))
        oov_id = None
        if self.oov_token is not None:
            oov_id = self.vocab[self.oov_token]
            self.vectors[oov_id] = np.random.uniform(-0.05, 0.05,
                                                     self.vector_len)
        return oov_id

    def _replace_oov_tokens(self, oov_token_id):
        &#34;&#34;&#34;
        Replace the vectors of words in vocab not found in the loaded file,
        with the vector of the oov token.

        Parameters
        ----------
        oov_token_id : int
            The index in vectors to the vector of the oov token.
        &#34;&#34;&#34;
        zeros_vector = np.zeros(shape=(self.vectors.shape[-1],))
        oov_vector = self.vectors[oov_token_id]
        for word, idx in self.vocab.items():
            if idx == 0:
                continue
            if np.array_equal(self.vectors[idx], zeros_vector):
                self.vectors[idx] = oov_vector

    @abc.abstractmethod
    def load(self, file_path):
        &#34;&#34;&#34;
        Read word vectors.

        Parameters
        ----------
        file_path : str
            File name to the glove file.
        &#34;&#34;&#34;
        raise NotImplementedError

    def get_embedding_layer(self, input_length, embedding_dim=None, **kwargs):
        &#34;&#34;&#34;
        Builds an Embedding layer.

        If we have loaded word vectors from a file, then those vectors will
        be used as weights, otherwise weights will be initialized with zero
        vectors and the `trainable` parameter will be set to True.

        Parameters
        ----------
        input_length : int
            The value to set in the `input_length` parameter of the
            `Embedding` layer.

        embedding_dim : int or None, default None
            In case of creating an Embedding layer with trainable weights
            (we haven&#39;t call the load method), this parameters should be set
            to the desired embedding dimensions. If there are loaded vectors,
            this parameters is ignored.

        kwargs
            Additional keyword arguments to pass to the Keras `Embedding` layer.

        Returns
        -------
        Embedding
            A Keras `Embedding` layer.

        Raises
        ------
        ValueError
            In case we haven&#39;t load word vectors and `embedding_dim` is None.
        &#34;&#34;&#34;
        if self.vector_len == 0:
            if embedding_dim is None:
                raise ValueError(
                    &#39;You have to give a value to the `embedding_dim` parameter&#39;
                )
            vector_len = embedding_dim
            kwargs[&#39;trainable&#39;] = True
        else:
            vector_len = self.vector_len
        layer = Embedding(
            input_dim=len(self.vocab),
            output_dim=vector_len,
            input_length=input_length,
            weights=[self.vectors],
            **kwargs)
        return layer

    def __getitem__(self, item):
        word_id = self.vocab[item]
        return self.vectors[word_id]

    def __getstate__(self):
        # `logger` object is not pickable. So we remove it.
        state = self.__dict__.copy()
        if &#39;logger&#39; in state:
            del state[&#39;logger&#39;]
        return state

    def __setstate__(self, state):
        # Unpickling the object, we don&#39;t have a logger instance which is
        # used in some methods. So we create a new one.
        self.__dict__ = state
        self.__dict__[&#39;logger&#39;] = logging.getLogger(self.__class__.__name__)


class Glove(WordVectors):
    &#34;&#34;&#34;
    Load word vectors from a Glove file.
    &#34;&#34;&#34;

    def __init__(self, vocab, oov_token=None):
        super().__init__(vocab, oov_token)

    def load(self, file_path):
        &#34;&#34;&#34;
        Load word vectors from a Glove file.
        
        Parameters
        ----------
        file_path : str
            The path to the file.
        &#34;&#34;&#34;
        line_no = 1
        with open(file_path, &#39;r&#39;) as f:
            self.logger.info(f&#39;Loading word vectors from file &#34;{file_path}&#34;.&#39;)
            # Read the first line to find the vectors length.
            line = f.readline()
            values = line.split()
            word = values[0]
            self.vector_len = len(values[1:])
            # Create the vocab&#39;s vector array.
            oov_id = self._init_vectors()
            # Initialize progress bar.
            progbar = Progbar(len(self.vocab) - 1)  # -PAD
            found = 1
            # Proceed with the rest file.
            while True:
                if word in self.vocab:
                    word_id = self.vocab[word]
                    vector = np.asarray(values[1:], dtype=&#39;float32&#39;)
                    self.vectors[word_id] = vector
                    progbar.update(found)
                    found += 1
                    if found == len(self.vocab):
                        progbar.update(len(self.vocab) - 1)
                        break
                try:
                    line = f.readline()
                    line_no += 1
                    values = line.split()
                    word = values[0]
                except (EOFError, IndexError):
                    progbar.update(len(self.vocab) - 1)
                    break

            if oov_id is not None:
                self._replace_oov_tokens(oov_id)


class W2V(WordVectors):
    &#34;&#34;&#34; Load word vectors from a Word2Vec file. &#34;&#34;&#34;

    def __init__(self, vocab, oov_token=None):
        super().__init__(vocab, oov_token)

    def load(self, file_path, binary=False):
        &#34;&#34;&#34;
        Load word vectors from a binary or text word2vec file.

        Parameters
        ----------
        file_path : str
            The path to the file.

        binary : bool, default False
            If the file is text set default to False, otherwise to True.

        Notes
        -----
        The code to load binary files is based on the code found here:
        https://gist.github.com/ottokart/673d82402ad44e69df85
        &#34;&#34;&#34;
        if binary:
            return self._load_binary_word2vec(file_path)
        else:
            return self._load_text_word2vec(file_path)

    def _load_binary_word2vec(self, file_path):
        &#34;&#34;&#34; Load word vectors from a binary word2vec file. &#34;&#34;&#34;
        float_size = 4  # 32bit float
        with open(file_path, &#39;rb&#39;) as f:
            c = None
            # read the header
            header = &#34;&#34;
            while c != &#34;\n&#34;:
                c = f.read(1).decode(&#39;utf-8&#39;)
                header += c

            num_vectors, self.vector_len = (int(x) for x in header.split())
            self.logger.info(f&#39;Loading {num_vectors} word vectors from file &#39;
                             f&#39;&#34;{file_path}&#34;.&#39;)
            oov_id = self._init_vectors()
            progbar = Progbar(len(self.vocab) - 1)
            found = 1
            for i in range(num_vectors):
                word = &#34;&#34;
                while True:
                    c = f.read(1).decode(&#39;utf-8&#39;)
                    if c == &#34; &#34;:
                        break
                    word += c
                word = word.strip()
                binary_vector = f.read(float_size * self.vector_len)
                if word in self.vocab:
                    word_vector = np.asarray([
                        struct.unpack_from(&#39;f&#39;, binary_vector, i)[0]
                        for i in range(0, len(binary_vector), float_size)
                    ])
                    word_id = self.vocab[word]
                    self.vectors[word_id] = word_vector
                    found += 1
                progbar.update(found)
            if found &lt; len(self.vocab):
                progbar.update(len(self.vocab) - 1)

            if oov_id is not None:
                self._replace_oov_tokens(oov_id)

    def _load_text_word2vec(self, file_path):
        with open(file_path, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
            line = f.readline()
            num_vectors, self.vector_len = (int(x) for x in line.split())
            self.logger.info(f&#39;Loading {num_vectors} word vectors from file &#39;
                             f&#39;&#34;{file_path}&#34;.&#39;)
            oov_id = self._init_vectors()
            found = 1
            progbar = Progbar(len(self.vocab) - 1)
            for i in range(num_vectors):
                line = f.readline()
                values = line.split()
                word = values[0]
                word_vector = np.asarray(values[1:], dtype=&#39;float32&#39;)
                if word in self.vocab:
                    word_id = self.vocab[word]
                    self.vectors[word_id] = word_vector
                    found += 1
                progbar.update(found)
            if found &lt; len(self.vocab):
                progbar.update(len(self.vocab) - 1)

            if oov_id is not None:
                self._replace_oov_tokens(oov_id)


if __name__ == &#39;__main__&#39;:
    import doctest

    doctest.testmod()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="keras_nlp.mappings.word_vectors.Glove"><code class="flex name class">
<span>class <span class="ident">Glove</span></span>
<span>(</span><span>vocab, oov_token=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Load word vectors from a Glove file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vocab</code></strong> :&ensp;<code>dict</code></dt>
<dd>A token to id mapping.</dd>
<dt><strong><code>oov_token</code></strong> :&ensp;<code>str</code> or <code>None</code>, default <code>None</code></dt>
<dd>If the vocabulary has an out-of-vocabulary token, then this
parameter should have the key of that token. A random vector is
create with uniform distribution in range [-0.05, 0.05].</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Glove(WordVectors):
    &#34;&#34;&#34;
    Load word vectors from a Glove file.
    &#34;&#34;&#34;

    def __init__(self, vocab, oov_token=None):
        super().__init__(vocab, oov_token)

    def load(self, file_path):
        &#34;&#34;&#34;
        Load word vectors from a Glove file.
        
        Parameters
        ----------
        file_path : str
            The path to the file.
        &#34;&#34;&#34;
        line_no = 1
        with open(file_path, &#39;r&#39;) as f:
            self.logger.info(f&#39;Loading word vectors from file &#34;{file_path}&#34;.&#39;)
            # Read the first line to find the vectors length.
            line = f.readline()
            values = line.split()
            word = values[0]
            self.vector_len = len(values[1:])
            # Create the vocab&#39;s vector array.
            oov_id = self._init_vectors()
            # Initialize progress bar.
            progbar = Progbar(len(self.vocab) - 1)  # -PAD
            found = 1
            # Proceed with the rest file.
            while True:
                if word in self.vocab:
                    word_id = self.vocab[word]
                    vector = np.asarray(values[1:], dtype=&#39;float32&#39;)
                    self.vectors[word_id] = vector
                    progbar.update(found)
                    found += 1
                    if found == len(self.vocab):
                        progbar.update(len(self.vocab) - 1)
                        break
                try:
                    line = f.readline()
                    line_no += 1
                    values = line.split()
                    word = values[0]
                except (EOFError, IndexError):
                    progbar.update(len(self.vocab) - 1)
                    break

            if oov_id is not None:
                self._replace_oov_tokens(oov_id)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="keras_nlp.mappings.word_vectors.WordVectors" href="#keras_nlp.mappings.word_vectors.WordVectors">WordVectors</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="keras_nlp.mappings.word_vectors.Glove.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, file_path)</span>
</code></dt>
<dd>
<section class="desc"><p>Load word vectors from a Glove file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the file.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, file_path):
    &#34;&#34;&#34;
    Load word vectors from a Glove file.
    
    Parameters
    ----------
    file_path : str
        The path to the file.
    &#34;&#34;&#34;
    line_no = 1
    with open(file_path, &#39;r&#39;) as f:
        self.logger.info(f&#39;Loading word vectors from file &#34;{file_path}&#34;.&#39;)
        # Read the first line to find the vectors length.
        line = f.readline()
        values = line.split()
        word = values[0]
        self.vector_len = len(values[1:])
        # Create the vocab&#39;s vector array.
        oov_id = self._init_vectors()
        # Initialize progress bar.
        progbar = Progbar(len(self.vocab) - 1)  # -PAD
        found = 1
        # Proceed with the rest file.
        while True:
            if word in self.vocab:
                word_id = self.vocab[word]
                vector = np.asarray(values[1:], dtype=&#39;float32&#39;)
                self.vectors[word_id] = vector
                progbar.update(found)
                found += 1
                if found == len(self.vocab):
                    progbar.update(len(self.vocab) - 1)
                    break
            try:
                line = f.readline()
                line_no += 1
                values = line.split()
                word = values[0]
            except (EOFError, IndexError):
                progbar.update(len(self.vocab) - 1)
                break

        if oov_id is not None:
            self._replace_oov_tokens(oov_id)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="keras_nlp.mappings.word_vectors.WordVectors" href="#keras_nlp.mappings.word_vectors.WordVectors">WordVectors</a></b></code>:
<ul class="hlist">
<li><code><a title="keras_nlp.mappings.word_vectors.WordVectors.get_embedding_layer" href="#keras_nlp.mappings.word_vectors.WordVectors.get_embedding_layer">get_embedding_layer</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="keras_nlp.mappings.word_vectors.W2V"><code class="flex name class">
<span>class <span class="ident">W2V</span></span>
<span>(</span><span>vocab, oov_token=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Load word vectors from a Word2Vec file. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vocab</code></strong> :&ensp;<code>dict</code></dt>
<dd>A token to id mapping.</dd>
<dt><strong><code>oov_token</code></strong> :&ensp;<code>str</code> or <code>None</code>, default <code>None</code></dt>
<dd>If the vocabulary has an out-of-vocabulary token, then this
parameter should have the key of that token. A random vector is
create with uniform distribution in range [-0.05, 0.05].</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class W2V(WordVectors):
    &#34;&#34;&#34; Load word vectors from a Word2Vec file. &#34;&#34;&#34;

    def __init__(self, vocab, oov_token=None):
        super().__init__(vocab, oov_token)

    def load(self, file_path, binary=False):
        &#34;&#34;&#34;
        Load word vectors from a binary or text word2vec file.

        Parameters
        ----------
        file_path : str
            The path to the file.

        binary : bool, default False
            If the file is text set default to False, otherwise to True.

        Notes
        -----
        The code to load binary files is based on the code found here:
        https://gist.github.com/ottokart/673d82402ad44e69df85
        &#34;&#34;&#34;
        if binary:
            return self._load_binary_word2vec(file_path)
        else:
            return self._load_text_word2vec(file_path)

    def _load_binary_word2vec(self, file_path):
        &#34;&#34;&#34; Load word vectors from a binary word2vec file. &#34;&#34;&#34;
        float_size = 4  # 32bit float
        with open(file_path, &#39;rb&#39;) as f:
            c = None
            # read the header
            header = &#34;&#34;
            while c != &#34;\n&#34;:
                c = f.read(1).decode(&#39;utf-8&#39;)
                header += c

            num_vectors, self.vector_len = (int(x) for x in header.split())
            self.logger.info(f&#39;Loading {num_vectors} word vectors from file &#39;
                             f&#39;&#34;{file_path}&#34;.&#39;)
            oov_id = self._init_vectors()
            progbar = Progbar(len(self.vocab) - 1)
            found = 1
            for i in range(num_vectors):
                word = &#34;&#34;
                while True:
                    c = f.read(1).decode(&#39;utf-8&#39;)
                    if c == &#34; &#34;:
                        break
                    word += c
                word = word.strip()
                binary_vector = f.read(float_size * self.vector_len)
                if word in self.vocab:
                    word_vector = np.asarray([
                        struct.unpack_from(&#39;f&#39;, binary_vector, i)[0]
                        for i in range(0, len(binary_vector), float_size)
                    ])
                    word_id = self.vocab[word]
                    self.vectors[word_id] = word_vector
                    found += 1
                progbar.update(found)
            if found &lt; len(self.vocab):
                progbar.update(len(self.vocab) - 1)

            if oov_id is not None:
                self._replace_oov_tokens(oov_id)

    def _load_text_word2vec(self, file_path):
        with open(file_path, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
            line = f.readline()
            num_vectors, self.vector_len = (int(x) for x in line.split())
            self.logger.info(f&#39;Loading {num_vectors} word vectors from file &#39;
                             f&#39;&#34;{file_path}&#34;.&#39;)
            oov_id = self._init_vectors()
            found = 1
            progbar = Progbar(len(self.vocab) - 1)
            for i in range(num_vectors):
                line = f.readline()
                values = line.split()
                word = values[0]
                word_vector = np.asarray(values[1:], dtype=&#39;float32&#39;)
                if word in self.vocab:
                    word_id = self.vocab[word]
                    self.vectors[word_id] = word_vector
                    found += 1
                progbar.update(found)
            if found &lt; len(self.vocab):
                progbar.update(len(self.vocab) - 1)

            if oov_id is not None:
                self._replace_oov_tokens(oov_id)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="keras_nlp.mappings.word_vectors.WordVectors" href="#keras_nlp.mappings.word_vectors.WordVectors">WordVectors</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="keras_nlp.mappings.word_vectors.W2V.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, file_path, binary=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Load word vectors from a binary or text word2vec file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the file.</dd>
<dt><strong><code>binary</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>If the file is text set default to False, otherwise to True.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The code to load binary files is based on the code found here:
<a href="https://gist.github.com/ottokart/673d82402ad44e69df85">https://gist.github.com/ottokart/673d82402ad44e69df85</a></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, file_path, binary=False):
    &#34;&#34;&#34;
    Load word vectors from a binary or text word2vec file.

    Parameters
    ----------
    file_path : str
        The path to the file.

    binary : bool, default False
        If the file is text set default to False, otherwise to True.

    Notes
    -----
    The code to load binary files is based on the code found here:
    https://gist.github.com/ottokart/673d82402ad44e69df85
    &#34;&#34;&#34;
    if binary:
        return self._load_binary_word2vec(file_path)
    else:
        return self._load_text_word2vec(file_path)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="keras_nlp.mappings.word_vectors.WordVectors" href="#keras_nlp.mappings.word_vectors.WordVectors">WordVectors</a></b></code>:
<ul class="hlist">
<li><code><a title="keras_nlp.mappings.word_vectors.WordVectors.get_embedding_layer" href="#keras_nlp.mappings.word_vectors.WordVectors.get_embedding_layer">get_embedding_layer</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="keras_nlp.mappings.word_vectors.WordVectors"><code class="flex name class">
<span>class <span class="ident">WordVectors</span></span>
<span>(</span><span>vocab, oov_token=None)</span>
</code></dt>
<dd>
<section class="desc"><p>A class to define common methods and functionality to load word vectors
from different formats.</p>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; import tempfile
&gt;&gt;&gt; from keras_nlp import Glove, WordVectorizer
&gt;&gt;&gt; vectors_file = tempfile.NamedTemporaryFile()
&gt;&gt;&gt; vectors_file.write(b'phasellus 0.1 -0.3 0.2\n')  #doctest:+SKIP
&gt;&gt;&gt; vectors_file.write(b'fermentum 0.2 0.1 -0.1\n')  #doctest:+SKIP
&gt;&gt;&gt; vectors_file.seek(0)  #doctest:+SKIP
&gt;&gt;&gt; texts = ['Phasellus fermentum tellus eget libero sodales varius.',
... 'In vestibulum erat nec nulla porttitor.']
&gt;&gt;&gt; word_vectorizer = WordVectorizer(oov_token='_UNK_', verbose=0)
&gt;&gt;&gt; word_vectorizer.fit_on_texts(texts)
&gt;&gt;&gt; glove = Glove(word_vectorizer.token2id, word_vectorizer.oov_token)
&gt;&gt;&gt; glove.load(vectors_file.name)
</code></pre>
<p>Validate the loaded vectors.</p>
<pre><code>&gt;&gt;&gt; arr1 = glove['phasellus']
&gt;&gt;&gt; arr2 = np.asarray([0.1, -0.3, 0.2])
&gt;&gt;&gt; np.testing.assert_array_almost_equal(arr1, arr2, decimal=1)
</code></pre>
<p>Embedding layer input dim should match vocabulary number of tokens.</p>
<pre><code>&gt;&gt;&gt; embedding_layer = glove.get_embedding_layer(input_length=7)
&gt;&gt;&gt; assert embedding_layer.input_dim = word_vectorizer.num_tokens
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vocab</code></strong> :&ensp;<code>dict</code></dt>
<dd>A token to id mapping.</dd>
<dt><strong><code>oov_token</code></strong> :&ensp;<code>str</code> or <code>None</code>, default <code>None</code></dt>
<dd>If the vocabulary has an out-of-vocabulary token, then this
parameter should have the key of that token. A random vector is
create with uniform distribution in range [-0.05, 0.05].</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WordVectors:
    &#34;&#34;&#34;
    A class to define common methods and functionality to load word vectors
    from different formats.

    Examples
    --------
    &gt;&gt;&gt; import tempfile
    &gt;&gt;&gt; from keras_nlp import Glove, WordVectorizer
    &gt;&gt;&gt; vectors_file = tempfile.NamedTemporaryFile()
    &gt;&gt;&gt; vectors_file.write(b&#39;phasellus 0.1 -0.3 0.2\\n&#39;)  #doctest:+SKIP
    &gt;&gt;&gt; vectors_file.write(b&#39;fermentum 0.2 0.1 -0.1\\n&#39;)  #doctest:+SKIP
    &gt;&gt;&gt; vectors_file.seek(0)  #doctest:+SKIP
    &gt;&gt;&gt; texts = [&#39;Phasellus fermentum tellus eget libero sodales varius.&#39;,
    ... &#39;In vestibulum erat nec nulla porttitor.&#39;]
    &gt;&gt;&gt; word_vectorizer = WordVectorizer(oov_token=&#39;_UNK_&#39;, verbose=0)
    &gt;&gt;&gt; word_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; glove = Glove(word_vectorizer.token2id, word_vectorizer.oov_token)
    &gt;&gt;&gt; glove.load(vectors_file.name)

    Validate the loaded vectors.
    &gt;&gt;&gt; arr1 = glove[&#39;phasellus&#39;]
    &gt;&gt;&gt; arr2 = np.asarray([0.1, -0.3, 0.2])
    &gt;&gt;&gt; np.testing.assert_array_almost_equal(arr1, arr2, decimal=1)

    Embedding layer input dim should match vocabulary number of tokens.
    &gt;&gt;&gt; embedding_layer = glove.get_embedding_layer(input_length=7)
    &gt;&gt;&gt; assert embedding_layer.input_dim = word_vectorizer.num_tokens
    &#34;&#34;&#34;

    def __init__(self, vocab, oov_token=None):
        &#34;&#34;&#34;
        Parameters
        ----------
        vocab : dict
            A token to id mapping.

        oov_token : str or None, default None
            If the vocabulary has an out-of-vocabulary token, then this
            parameter should have the key of that token. A random vector is
            create with uniform distribution in range [-0.05, 0.05].
        &#34;&#34;&#34;
        self.vocab = vocab
        self.oov_token = oov_token
        self.vector_len = 0
        self.vectors = None

        self.logger = logging.getLogger(self.__class__.__name__)

    def _init_vectors(self):
        &#34;&#34;&#34;
        Initialize vectors array and insert a random vector in case of having
        an out-of-vocabulary token.
        &#34;&#34;&#34;
        self.vectors = np.zeros(shape=(len(self.vocab), self.vector_len))
        oov_id = None
        if self.oov_token is not None:
            oov_id = self.vocab[self.oov_token]
            self.vectors[oov_id] = np.random.uniform(-0.05, 0.05,
                                                     self.vector_len)
        return oov_id

    def _replace_oov_tokens(self, oov_token_id):
        &#34;&#34;&#34;
        Replace the vectors of words in vocab not found in the loaded file,
        with the vector of the oov token.

        Parameters
        ----------
        oov_token_id : int
            The index in vectors to the vector of the oov token.
        &#34;&#34;&#34;
        zeros_vector = np.zeros(shape=(self.vectors.shape[-1],))
        oov_vector = self.vectors[oov_token_id]
        for word, idx in self.vocab.items():
            if idx == 0:
                continue
            if np.array_equal(self.vectors[idx], zeros_vector):
                self.vectors[idx] = oov_vector

    @abc.abstractmethod
    def load(self, file_path):
        &#34;&#34;&#34;
        Read word vectors.

        Parameters
        ----------
        file_path : str
            File name to the glove file.
        &#34;&#34;&#34;
        raise NotImplementedError

    def get_embedding_layer(self, input_length, embedding_dim=None, **kwargs):
        &#34;&#34;&#34;
        Builds an Embedding layer.

        If we have loaded word vectors from a file, then those vectors will
        be used as weights, otherwise weights will be initialized with zero
        vectors and the `trainable` parameter will be set to True.

        Parameters
        ----------
        input_length : int
            The value to set in the `input_length` parameter of the
            `Embedding` layer.

        embedding_dim : int or None, default None
            In case of creating an Embedding layer with trainable weights
            (we haven&#39;t call the load method), this parameters should be set
            to the desired embedding dimensions. If there are loaded vectors,
            this parameters is ignored.

        kwargs
            Additional keyword arguments to pass to the Keras `Embedding` layer.

        Returns
        -------
        Embedding
            A Keras `Embedding` layer.

        Raises
        ------
        ValueError
            In case we haven&#39;t load word vectors and `embedding_dim` is None.
        &#34;&#34;&#34;
        if self.vector_len == 0:
            if embedding_dim is None:
                raise ValueError(
                    &#39;You have to give a value to the `embedding_dim` parameter&#39;
                )
            vector_len = embedding_dim
            kwargs[&#39;trainable&#39;] = True
        else:
            vector_len = self.vector_len
        layer = Embedding(
            input_dim=len(self.vocab),
            output_dim=vector_len,
            input_length=input_length,
            weights=[self.vectors],
            **kwargs)
        return layer

    def __getitem__(self, item):
        word_id = self.vocab[item]
        return self.vectors[word_id]

    def __getstate__(self):
        # `logger` object is not pickable. So we remove it.
        state = self.__dict__.copy()
        if &#39;logger&#39; in state:
            del state[&#39;logger&#39;]
        return state

    def __setstate__(self, state):
        # Unpickling the object, we don&#39;t have a logger instance which is
        # used in some methods. So we create a new one.
        self.__dict__ = state
        self.__dict__[&#39;logger&#39;] = logging.getLogger(self.__class__.__name__)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="keras_nlp.mappings.word_vectors.Glove" href="#keras_nlp.mappings.word_vectors.Glove">Glove</a></li>
<li><a title="keras_nlp.mappings.word_vectors.W2V" href="#keras_nlp.mappings.word_vectors.W2V">W2V</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="keras_nlp.mappings.word_vectors.WordVectors.get_embedding_layer"><code class="name flex">
<span>def <span class="ident">get_embedding_layer</span></span>(<span>self, input_length, embedding_dim=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Builds an Embedding layer.</p>
<p>If we have loaded word vectors from a file, then those vectors will
be used as weights, otherwise weights will be initialized with zero
vectors and the <code>trainable</code> parameter will be set to True.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_length</code></strong> :&ensp;<code>int</code></dt>
<dd>The value to set in the <code>input_length</code> parameter of the
<code>Embedding</code> layer.</dd>
<dt><strong><code>embedding_dim</code></strong> :&ensp;<code>int</code> or <code>None</code>, default <code>None</code></dt>
<dd>In case of creating an Embedding layer with trainable weights
(we haven't call the load method), this parameters should be set
to the desired embedding dimensions. If there are loaded vectors,
this parameters is ignored.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional keyword arguments to pass to the Keras <code>Embedding</code> layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Embedding</code></dt>
<dd>A Keras <code>Embedding</code> layer.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>In case we haven't load word vectors and <code>embedding_dim</code> is None.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_embedding_layer(self, input_length, embedding_dim=None, **kwargs):
    &#34;&#34;&#34;
    Builds an Embedding layer.

    If we have loaded word vectors from a file, then those vectors will
    be used as weights, otherwise weights will be initialized with zero
    vectors and the `trainable` parameter will be set to True.

    Parameters
    ----------
    input_length : int
        The value to set in the `input_length` parameter of the
        `Embedding` layer.

    embedding_dim : int or None, default None
        In case of creating an Embedding layer with trainable weights
        (we haven&#39;t call the load method), this parameters should be set
        to the desired embedding dimensions. If there are loaded vectors,
        this parameters is ignored.

    kwargs
        Additional keyword arguments to pass to the Keras `Embedding` layer.

    Returns
    -------
    Embedding
        A Keras `Embedding` layer.

    Raises
    ------
    ValueError
        In case we haven&#39;t load word vectors and `embedding_dim` is None.
    &#34;&#34;&#34;
    if self.vector_len == 0:
        if embedding_dim is None:
            raise ValueError(
                &#39;You have to give a value to the `embedding_dim` parameter&#39;
            )
        vector_len = embedding_dim
        kwargs[&#39;trainable&#39;] = True
    else:
        vector_len = self.vector_len
    layer = Embedding(
        input_dim=len(self.vocab),
        output_dim=vector_len,
        input_length=input_length,
        weights=[self.vectors],
        **kwargs)
    return layer</code></pre>
</details>
</dd>
<dt id="keras_nlp.mappings.word_vectors.WordVectors.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, file_path)</span>
</code></dt>
<dd>
<section class="desc"><p>Read word vectors.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>File name to the glove file.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def load(self, file_path):
    &#34;&#34;&#34;
    Read word vectors.

    Parameters
    ----------
    file_path : str
        File name to the glove file.
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="keras_nlp.mappings" href="index.html">keras_nlp.mappings</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="keras_nlp.mappings.word_vectors.Glove" href="#keras_nlp.mappings.word_vectors.Glove">Glove</a></code></h4>
<ul class="">
<li><code><a title="keras_nlp.mappings.word_vectors.Glove.load" href="#keras_nlp.mappings.word_vectors.Glove.load">load</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="keras_nlp.mappings.word_vectors.W2V" href="#keras_nlp.mappings.word_vectors.W2V">W2V</a></code></h4>
<ul class="">
<li><code><a title="keras_nlp.mappings.word_vectors.W2V.load" href="#keras_nlp.mappings.word_vectors.W2V.load">load</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="keras_nlp.mappings.word_vectors.WordVectors" href="#keras_nlp.mappings.word_vectors.WordVectors">WordVectors</a></code></h4>
<ul class="">
<li><code><a title="keras_nlp.mappings.word_vectors.WordVectors.get_embedding_layer" href="#keras_nlp.mappings.word_vectors.WordVectors.get_embedding_layer">get_embedding_layer</a></code></li>
<li><code><a title="keras_nlp.mappings.word_vectors.WordVectors.load" href="#keras_nlp.mappings.word_vectors.WordVectors.load">load</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>