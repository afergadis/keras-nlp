<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>keras_nlp.preprocessing.text API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>keras_nlp.preprocessing.text</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging
import abc
import re
import numpy as np
from collections import Counter
from keras.utils.generic_utils import Progbar
from keras_nlp.preprocessing import segment


def calc_stats(array):
    &#34;&#34;&#34;
    Calculate basic statistics for a list of numbers.

    Parameters
    ----------
    array : list, 1d array-like
        A list of numbers.

    Returns
    -------
    dict
        The dictionary has keys: min, max, median, mean, std, 25-percentile,
        50-percentile, 75-percentile.

    &#34;&#34;&#34;
    array = np.asarray(array)
    stats = {
        &#39;min&#39;: np.amin(array),
        &#39;max&#39;: np.amax(array),
        &#39;median&#39;: np.median(array),
        &#39;mean&#39;: np.mean(array),
        &#39;std&#39;: np.std(array),
        &#39;25-percentile&#39;: np.percentile(array, 25),
        &#39;50-percentile&#39;: np.percentile(array, 50),
        &#39;75-percentile&#39;: np.percentile(array, 75)
    }
    return stats


class Vectorizer:
    &#34;&#34;&#34;
    An abstract base class for different types of Vectorizers.
    &#34;&#34;&#34;

    def __init__(self,
                 word_tokenize=None,
                 num_tokens=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        num_tokens : int or None, default None
            The maximum number of tokens in the vocabulary of the vectorizer.
            If is set to None, then all the tokens found using `fit_on_texts`
            method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define it the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.
        &#34;&#34;&#34;
        if word_tokenize is None:
            self.word_tokenize = segment.word_tokenize
        self.num_tokens = num_tokens
        if filters is not None:
            self.filters = &#39;[&#39; + re.escape(filters) + &#39;]&#39;
        else:
            self.filters = None
        self.lower = lower
        self.oov_token = oov_token

        self.token2id = None
        self.id2token = None
        self.token_counts = Counter()

        self.verbose = verbose
        self.logger = logging.getLogger(self.__class__.__name__)

    def _apply_filters(self, text):
        &#34;&#34;&#34; Lowers and removes filters from the text. &#34;&#34;&#34;
        if self.lower:
            text = text.lower()

        if self.filters is not None:
            text = re.sub(self.filters, &#39; &#39;, text)

        return text

    def _bag_of_tokens(self):
        &#34;&#34;&#34;
        Create the `self.token2id` and `self.id2token` vocabularies.
        &#34;&#34;&#34;
        if self.token2id is None:
            raise ValueError(&#39;Please use the `fit_on_texts` method first.&#39;)
        if self.num_tokens is not None:
            tokens = self.token_counts.most_common(self.num_tokens)
        else:
            tokens = self.token_counts.most_common()

        if len(tokens) &gt; 0:
            # Tokens are pairs of (token, count). We keep only the token.
            tokens = [token[0] for token in tokens]
            for token in tokens:
                self.token2id[token] = len(self.token2id)
            self.id2token = {v: k for k, v in self.token2id.items()}
        self.num_tokens = len(self.token2id)

    def _init_vocab(self):
        self.token2id = {&#39;_PAD_&#39;: 0}
        if self.oov_token is not None:
            self.token2id[self.oov_token] = 1

    @abc.abstractmethod
    def fit_on_texts(self, texts):
        &#34;&#34;&#34;
        Creates a tokens vocabulary from the list of texts.

        Parameters
        ----------
        texts : list or list[list]
            Each list item can be a text representing a document or a list of
            texts.

        &#34;&#34;&#34;
        raise NotImplementedError()

    @abc.abstractmethod
    def _pad_vectors(self,
                     vectors,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        &#34;&#34;&#34;
        Pad vectors to 2D or 3D arrays.

        A `CharVectorizer` will output list[list] vectors where the outer list
        represent a text and the inner list the tokens. The method will
        ensure that all texts have the same number of tokens and all tokens
        have the same number of characters.
        A `SentWordVectorizer` will output list[list] vectors where the outer
        list represent a sentence and the inner list the tokens. The method will
        ensure that all texts have the same number of sentences and all
        sentences have the same number of tokens.
        A `SentCharVectorizer` will output list[list[list]] vectors. The
        outer list represents the number of sentences of a text, the 2nd
        represents the number of tokens and the 3d the number of characters
        per token.

        Parameters
        ----------
        vectors : list
            A list[list] or list[list[list]] of ids depending on the instance
            of the Vectorizer.

        shape : int, list or tuple
            The target shape of the vectors padded or truncated depending
            on the `shape`&#39;s 1st dimension and the length of `vectors`.

        padding : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            Add `pad_value`s to vectors, based on the selected method, in
            order to match the target `shape`.

        truncating : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            Delete values, based on the selected method, from `vectors` in
            order to match the target `shape`.

        pad_value : int, default 0
            In case of padding, the value to use.

        Returns
        -------
        ndarray
            An array of shape `shape`.
        &#34;&#34;&#34;
        raise NotImplementedError()

    @staticmethod
    def _pad_sequences(sequences,
                       maxlen,
                       padding=&#39;pre&#39;,
                       truncating=&#39;pre&#39;,
                       pad_value=0):
        array = np.full(shape=(len(sequences), maxlen), fill_value=pad_value)
        for idx, seq in enumerate(sequences):
            if not len(seq):
                continue
            if len(seq) &lt; maxlen:
                if padding == &#39;pre&#39;:
                    array[idx, -len(seq):] = seq
                elif padding == &#39;post&#39;:
                    array[idx, :len(seq)] = seq
                else:
                    raise ValueError(
                        f&#39;Unknown option `{padding}` for padding.&#39;)
            else:
                if truncating == &#39;pre&#39;:
                    array[idx, :] = seq[-maxlen:]
                elif truncating == &#39;post&#39;:
                    array[idx, :] = seq[:maxlen]
                else:
                    if not isinstance(truncating, (tuple, list)):
                        raise ValueError(
                            f&#39;Unknown option `{truncating}` for truncating.&#39;)
                    if len(truncating) != 2:
                        raise ValueError(
                            &#39;`truncating` is expected to be a tuple &#39;
                            &#39;with two numbers; one for the &#39;
                            &#39;percentage of text to keep from the &#39;
                            &#39;beginning and one for the end.&#39;)
                    if sum(truncating) != 1:
                        raise ValueError(&#39;`truncating` should sum to 1.&#39;)
                    pre = round(maxlen * truncating[0])
                    post = int(np.ceil(maxlen * truncating[1]))
                    array[idx, :] = seq[:pre] + seq[-post:]
        return array

    @staticmethod
    def _reshape(vectors,
                 num_rows,
                 padding=&#39;pre&#39;,
                 truncating=&#39;pre&#39;,
                 pad_value=0):
        &#34;&#34;&#34;
        Reshape `vectors` to `(num_rows, vectors.shape[1:])`.

        This function is used in order to cut or add rows to the `vectors`
        array, keep the rest dimensions.  This is usefull to make all documents
        of a collection to have the same number of (words, characters) or
        (sentences, words) in case of 2D `vectors`, or (sentences, words,
        characters) in case of 3D `vectors`.

        Parameters
        ----------
        vectors : ndarray
            A 2D or 3D array, depending on the calling instance of `Vectorizer`

        num_rows : int
            The new number or rows of the `vectors`, padded or truncated
            if needed.

        padding: str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            The padding method to use in case `num_rows &gt; sequences.shape[1]`

        truncating : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            The truncating method to use in case `num_rows &lt; sequences.shape[1]`

        pad_value: int, default 0
            The value to use in case of padding.

        Returns
        -------
        ndarray

        Raises
        ------
        ValueError
            `padding` or `truncating` values are not recognized.
        &#34;&#34;&#34;
        target_shape = [num_rows] + list(vectors.shape[1:])
        array = np.full(target_shape, fill_value=pad_value)
        if vectors.shape[0] &lt; target_shape[0]:
            if padding == &#39;pre&#39;:
                array[-vectors.shape[0]:, :] = vectors
            elif padding == &#39;post&#39;:
                array[:vectors.shape[0], :] = vectors
            else:
                raise ValueError(
                    &#39;Unknown option `{}` for padding.&#39;.format(padding))
        elif vectors.shape[0] &gt; target_shape[0]:
            if truncating == &#39;pre&#39;:
                array = vectors[-target_shape[0]:, :]
            elif truncating == &#39;post&#39;:
                array = vectors[:target_shape[0], :]
            else:
                if not isinstance(truncating, (tuple, list)):
                    raise ValueError(
                        f&#39;Unknown option `{truncating}` for truncating.&#39;)
                if len(truncating) != 2:
                    raise ValueError(&#39;`truncating` is expected to be a tuple &#39;
                                     &#39;with two numbers; one for the &#39;
                                     &#39;percentage of text to keep from the &#39;
                                     &#39;beginning and one for the end.&#39;)
                if sum(truncating) != 1:
                    raise ValueError(&#39;`truncating` should sum to 1.&#39;)
                pre = round(target_shape[0] * truncating[0])
                post = int(np.ceil(target_shape[0] * truncating[1]))
                # Mask the values between pre and post to False.
                mask = np.ones(len(vectors), dtype=bool)
                mask[range(pre, len(vectors) - post)] = False
                array = vectors[mask, ...]
        else:
            array = vectors
        return array

    def _tokenize_text(self, text):
        if isinstance(text, list):
            _text = []
            for sentence in text:
                sentence = self._apply_filters(sentence)
                words = self.word_tokenize(sentence)
                _text.append(words)
            tokenized_text = _text
        else:
            text = self._apply_filters(text)
            words = self.word_tokenize(text)
            tokenized_text = words

        return tokenized_text

    def _tokens_to_chars(self, tokens):
        &#34;&#34;&#34;
        For each token create a list of it&#39;s character ids.

        Parameters
        ----------
        tokens : list
            The list of tokens to get the character ids from.

        Yields
        ------
        list[list]
            Each list item corresponds to a token. Each token is a list of its
            character ids.
        &#34;&#34;&#34;
        for token in tokens:
            _chars = []
            for char in token:
                if char in self.token2id:
                    _chars.append(self.token2id[char])
                else:
                    if self.oov_token is not None:
                        _chars.append(self.token2id[self.oov_token])
            yield _chars

    @abc.abstractmethod
    def texts_to_vectors(self,
                         texts,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;post&#39;,
                         pad_value=None):
        &#34;&#34;&#34;
        Convert a list of texts to a 2D, 3D or 4D array.

        * A `WordVectorizer` will return a 2D array of shape `(len(texts),
        max_tokens)`.
        * A `CharVectorizer` will return a 3D array of shape `(len(texts),
        max_tokens, max_characters)`.
        * A `SentWordVectorizer` will return a 3D array of shape `(len(texts),
        max_sentences, max_tokens)`.
        * A `SentCharVectorizer` will return a 4D array of shape `(len(texts),
        max_sentences, max_tokens, max_characters)`.

        Parameters
        ----------
        texts : list
            The list of texts to convert. Each item represents a document.

        shape : tuple, list or None, default None
            * In case of `WordVectorizer` the shape defines the tuple
            `(max_tokens, )` per text.
            * In case of `CharVectorizer` the shape defines the tuple `(
            max_tokens, max_characters)`.
            * In case of `SentWordVectorizer` the shape defines the tuple `(
            max_sentences, max_tokens)`.
            * In case of `SentCharVectorizer` the shape defines the tuple `(
            max_sentences, max_tokens, max_characters)`.

        padding : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            Defines the padding method when the length of tokens/characters
            is less than `max_tokens`/`max_characters`. The filling value is
            the value of the `pad_value` parameter.

        truncating : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            Defines the cutting method when  the length of tokens/characters
            is larger than `max_tokens/characters`.

        pad_value : int, default None
            The value to use when padding is needed. If the value is `None`
            then the `token2id[&#39;_PAD_&#39;]` value is used which by default is 0.

        Returns
        -------
        ndarray : Numpy 2D array with shape `shape`

        Raises
        ------
        ValueError : `shape` len is not 2 and `shape` is not None.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def vectors_to_texts(self, vectors):
        &#34;&#34;&#34;
        Decode `vectors` array to texts using the Vectorizer&#39;s vocabulary.

        Parameters
        ----------
        vectors : ndarray
            The shape of `vectors` may be:

            * 2D `(num_of_texts, num_of_tokens)` from `WordVectorizer`;
            * 3D `(num_of_texts, num_of_sentences, num_of_words)` from
            `SentWordVectorizer`;
            * 3D `(num_of_texts, num_of_words, num_of_characters)` from
            `CharVectorizer`;
            a 4D `(num_of_texts, num_of_sentences, num_of_words,
            num_of_characters)` from `SentCharVectorizer`.

        Returns
        -------
        list
            The list has length of `num_of_texts`.

            * In case of 2D input, the list has the words of the texts.
            * In case of 3D input, the list has a list sentences. Each item of
            the nested list is a word in case of `SentWordVectorizer`,
            or the list has a list of words. Each item of the word&#39;s list is
            the characters of the words.
            * In case of 4D input, the output is a list of texts with their list
            of sentences. For each sentence a list of tokens where each token
            is a list of its characters.
        &#34;&#34;&#34;

        def decode_vector(vector):
            text = []
            for values in vector:
                tokens = [
                    self.id2token[t] for t in values
                    if t != self.token2id[&#39;_PAD_&#39;]
                ]
                if len(tokens) &gt; 0:
                    text.append(tokens)
            return text

        if self.id2token is None:
            raise ValueError(&#39;Please use `fit_on_texts` method first.&#39;)
        self.logger.info(&#39;Converting vectors to texts.&#39;)
        texts = []
        progbar = Progbar(vectors.shape[0], verbose=self.verbose)
        if vectors.ndim == 2:
            for doc in vectors:
                # Just to be able to update progress bar.
                text = decode_vector(doc.reshape(1, -1))
                # text is a list with one item; the words of the document.
                texts.append(text[0])
                progbar.update(len(texts))
        elif vectors.ndim == 3:
            for vector in vectors:
                decoded = decode_vector(vector)
                if len(decoded) &gt; 0:  # If len==0, it is padded.
                    texts.append(decoded)
                progbar.update(len(texts))
        elif vectors.ndim == 4:
            for doc in vectors:
                text = []
                for sents in doc:
                    decoded = decode_vector(sents)
                    if len(decoded) &gt; 0:  # If len==0, it is padded.
                        text.append(decoded)
                texts.append(text)
                progbar.update(len(texts))

        return texts

    @abc.abstractmethod
    def stats(self):
        &#34;&#34;&#34;
        Get statistics about the texts the Vectorizer is fitted on.

        Returns
        -------
        dict
        &#34;&#34;&#34;
        raise NotImplementedError()

    def __getstate__(self):
        # `logger` object is not pickable. So we remove it.
        state = self.__dict__.copy()
        if &#39;logger&#39; in state:
            del state[&#39;logger&#39;]
        return state

    def __setstate__(self, state):
        # Unpickling the object, we don&#39;t have a logger instance which is
        # used in some methods. So we create a new one.
        self.__dict__ = state
        self.__dict__[&#39;logger&#39;] = logging.getLogger(self.__class__.__name__)


class CharVectorizer(Vectorizer):
    &#34;&#34;&#34;
    Convert a list of texts to an array of vectors with shape `(num_of_texts,
    max_words, max_characters)`.

    For each text (document) keep a maximum number of words. Each word is
    represented with a list of `max_characters` ids.

    Examples
    --------
    &gt;&gt;&gt; char_vectorizer = CharVectorizer(oov_token=&#39;?&#39;, \
    characters=&#39;abcdefghijklmnopqrstuvwxyz&#39;, verbose=0)
    &gt;&gt;&gt; texts = [&#39;Phasellus fermentum tellus eget libero sodales varius.&#39;,
    ... &#39;In vestibulum erat nec nulla porttitor dignissim.&#39;]
    &gt;&gt;&gt; # In case `characters` are set, fit_on_texts it&#39;s secondary.
    &gt;&gt;&gt; char_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; docs = [&#39;Nam accumsan velit vel ligula convallis cursus.&#39;,
    ... &#39;Nulla porttitor felis risus, vitae facilisis massa consectetur id.&#39;]
    &gt;&gt;&gt; vectors = char_vectorizer.texts_to_vectors(docs)
    &gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_words, max_characters)
    (2, 9, 11)
    &gt;&gt;&gt; decoded = char_vectorizer.vectors_to_texts(vectors)
    &gt;&gt;&gt; print(decoded[0][:2])  # First 2 words of the 1st doc in docs.
    [[&#39;n&#39;, &#39;a&#39;, &#39;m&#39;], [&#39;a&#39;, &#39;c&#39;, &#39;c&#39;, &#39;u&#39;, &#39;m&#39;, &#39;s&#39;, &#39;a&#39;, &#39;n&#39;]]
    &gt;&gt;&gt; decoded_words = char_vectorizer.vectors_to_texts(vectors, True)
    &gt;&gt;&gt; print(decoded_words[0][:2])  # First 2 words of the 1st doc in docs.
    [&#39;nam&#39;, &#39;accumsan&#39;]
    &#34;&#34;&#34;

    def __init__(self,
                 word_tokenize=None,
                 characters=None,
                 num_chars=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        characters : str, list or None, default None
            If set then only the characters defined will be in the vectrorizer&#39;s
            vocabulary. Otherwise, the characters found in the texts during
            `fit_on_texts` method will be used.

        num_chars : int or None, default None
            The maximum number of characters in the vocabulary of the
            vectorizer. If is set to None, then all the characters found using
            `fit_on_texts` method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define if the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.

        Attributes
        ----------
        num_tokens : int
            The number of tokens (characters) in the vocabulary.
        num_chars : int
            Alias for the `num_tokens` attribute.
        words_stats : dict
            A dictionary with statistics about the tokens (words) of the texts
            (min/max/std/mean/median/percentiles of words among texts).
        chars_stats : dict
            A dictionary with statistics about the characters of the words.
            (min/max/std/mean/median/percentiles of characters among words).
        &#34;&#34;&#34;
        super().__init__(word_tokenize, num_chars, filters, lower, oov_token,
                         verbose)
        self.num_chars = num_chars

        if characters is None:
            self.characters = None
        else:
            self.characters = list(characters)
            self.token_counts.update(self.characters)

        self.words_stats = dict()
        self.chars_stats = dict()

        self.logger = logging.getLogger(self.__class__.__name__)

    def fit_on_texts(self, texts):
        self._init_vocab()
        self.logger.info(&#39;Creating vocabulary.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        # Text length counting words and word length counting characters.
        texts_len, words_len = [], []
        for i, text in enumerate(texts):
            tokens = self._tokenize_text(text)
            texts_len.append(len(tokens))
            words_len.extend([len(token) for token in tokens])
            if self.characters is None:
                chars = list(&#39;&#39;.join(tokens))
                self.token_counts.update(chars)
            progbar.update(i)
        progbar.update(len(texts))  # Finalize

        self._bag_of_tokens()
        self.num_chars = self.num_tokens

        self.words_stats = calc_stats(texts_len)
        self.chars_stats = calc_stats(words_len)

    def _pad_vectors(self,
                     sequences,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        max_words, max_characters = shape
        vectors = np.full(
            shape=(len(sequences), max_words, max_characters),
            fill_value=pad_value,
            dtype=int)
        self.logger.info(f&#39;Reshaping vectors to shape {shape}.&#39;)
        progbar = Progbar(len(sequences), verbose=self.verbose)
        for i, doc in enumerate(sequences):
            chars_vector = self._pad_sequences(
                doc,
                max_characters,
                padding=padding,
                truncating=truncating,
                pad_value=pad_value)
            vectors[i] = self._reshape(chars_vector, max_words, padding,
                                       truncating, pad_value)
            progbar.update(i)
        progbar.update(len(sequences))  # Finalize
        return vectors

    def texts_to_vectors(self,
                         texts: list,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;pre&#39;,
                         pad_value=None):
        if shape is not None:
            if len(shape) != 2:
                raise ValueError(
                    f&#39;The `shape` should be of rank 2 defining the&#39;
                    f&#39;maximum words per text and the maximum &#39;
                    f&#39;characters per word. Found a shape &#39;
                    f&#39;with rank {len(shape)}.&#39;)
        if pad_value is None:
            pad_value = self.token2id[&#39;_PAD_&#39;]
        _texts = []
        self.logger.info(&#39;Converting texts to vectors.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        for text in texts:
            text = self._apply_filters(text)
            words = self.word_tokenize(text)
            # In some rare cases, all the characters of a word have been
            # removed because of the filter characters. So the length of the
            # word is zero.
            if len(words) == 0:
                _words = [[0]]  # The list of characters in a word.
            else:
                _words = list(self._tokens_to_chars(words))
            _texts.append(_words)
            progbar.update(len(_texts))

        if shape is None:
            max_words = self.words_stats[&#39;max&#39;]
            max_characters = self.chars_stats[&#39;max&#39;]
            shape = (max_words, max_characters)
        vectors = self._pad_vectors(_texts, shape, padding, truncating,
                                    pad_value)
        return vectors

    def vectors_to_texts(self, vectors, as_words=False):
        texts = super().vectors_to_texts(vectors)
        if as_words:
            texts = [[&#39;&#39;.join(chars) for chars in words] for words in texts]
        return texts

    def stats(self):
        return self.words_stats, self.chars_stats

    def __str__(self):
        if self.token2id is None:
            msg = &#39;CharVectorizer(Vocab Size: 0)&#39;
        else:
            msg = &#39;CharVectorizer(Vocab Size: {}, Max Words: {}, &#39; \
                  &#39;Max Characters: {})&#39;.format(
                len(self.token2id), self.words_stats[&#39;max&#39;],
                self.chars_stats[&#39;max&#39;])
        return msg

    def __repr__(self):
        return self.__str__()


class WordVectorizer(Vectorizer):
    &#34;&#34;&#34;
    Convert a list of texts to an array of vectors with shape `(num_of_texts,
    max_words)`.

    Examples
    --------
    &gt;&gt;&gt; word_vectorizer = WordVectorizer(verbose=0)
    &gt;&gt;&gt; texts = [&#39;Phasellus fermentum tellus eget libero sodales varius.&#39;,
    ... &#39;In vestibulum erat nec nulla porttitor dignissim.&#39;]
    &gt;&gt;&gt; word_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; vectors = word_vectorizer.texts_to_vectors(texts)
    &gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_words)
    (2, 7)
    &gt;&gt;&gt; decoded = word_vectorizer.vectors_to_texts(vectors)
    &gt;&gt;&gt; print(decoded[0][:3])  # First 3 words of the 1st doc in docs.
    [&#39;phasellus&#39;, &#39;fermentum&#39;, &#39;tellus&#39;]
    &#34;&#34;&#34;

    def __init__(self,
                 word_tokenize=None,
                 num_words=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        num_words : int or None, default None
            The maximum number of words in the vocabulary of the
            vectorizer. If is set to None, then all the words found using
            `fit_on_texts` method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define if the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.

        Attributes
        ----------
        num_tokens : int
            The number of tokens (words) in the vocabulary.

        num_words : int
            Alias for the `num_tokens` attribute.

        words_stats : dict
            A dictionary with statistics about the tokens (words) of the texts
            (min/max/std/mean/median/percentiles of words among texts).
        &#34;&#34;&#34;
        super().__init__(word_tokenize, num_words, filters, lower, oov_token,
                         verbose)
        self.words_stats = dict()
        self.logger = logging.getLogger(self.__class__.__name__)

    def fit_on_texts(self, texts):
        self._init_vocab()
        self.logger.info(&#39;Creating vocabulary.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        docs_len = []
        for i, text in enumerate(texts):
            tokens = self._tokenize_text(text)
            docs_len.append(len(tokens))
            self.token_counts.update(tokens)
            progbar.update(i)
        progbar.update(len(texts))

        self._bag_of_tokens()
        self.words_stats = calc_stats(docs_len)

    def _pad_vectors(self,
                     vectors,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        pass

    def texts_to_vectors(self,
                         texts: list,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;pre&#39;,
                         pad_value=None):
        if shape is not None:
            if len(shape) != 1:
                raise ValueError(
                    f&#39;The `shape` should be of rank 1 defining the&#39;
                    f&#39;maximum words per text. Found a shape &#39;
                    f&#39;with rank {len(shape)}.&#39;)
        if pad_value is None:
            pad_value = self.token2id[&#39;_PAD_&#39;]
        _texts = []
        self.logger.info(&#39;Converting texts to vectors.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        for text in texts:
            _words = []
            text = self._apply_filters(text)
            words = self.word_tokenize(text)
            for word in words:
                if word in self.token2id:
                    _words.append(self.token2id[word])
                else:
                    if self.oov_token is not None:
                        _words.append(self.token2id[self.oov_token])
            _texts.append(_words)
            progbar.update(len(_texts))

        if shape is None:
            shape = (self.words_stats[&#39;max&#39;], )

        vectors = self._pad_sequences(
            _texts,
            shape[0],
            padding=padding,
            truncating=truncating,
            pad_value=pad_value)
        return vectors

    def stats(self):
        return self.words_stats

    def __str__(self):
        if self.token2id is None:
            msg = &#39;WordVectorizer(Vocab Size: 0)&#39;
        else:
            msg = &#39;&lt;WordVectorizer(Vocab Size: {}, Max Words: {})&#39;.format(
                len(self.token2id), self.words_stats[&#39;max&#39;])
        return msg

    def __repr__(self):
        return self.__str__()


class SentCharVectorizer(CharVectorizer):
    &#34;&#34;&#34;
    Convert a list of texts to an array of vectors with shape `(num_of_texts,
    max_sentences, max_words, max_characters)`.

    Examples
    --------
    &gt;&gt;&gt; sent_char_vectorizer = SentCharVectorizer(verbose=0)
    &gt;&gt;&gt; # Two documents. The fists with two sentences and the second with one.
    &gt;&gt;&gt; # The 1st document is already tokenized on sentences. Alternately, you
    &gt;&gt;&gt; # may pass a sent_tokenizer callable.
    &gt;&gt;&gt; texts = [[&#39;Phasellus fermentum tellus eget libero sodales varius.&#39;,
    ... &#39;In vestibulum erat nec nulla porttitor dignissim.&#39;],
    ... [&#39;Nam accumsan velit vel ligula convallis cursus.&#39;]]
    &gt;&gt;&gt; sent_char_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; vectors = sent_char_vectorizer.texts_to_vectors(texts)
    &gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_sentences, max_words, max_characters)
    (2, 2, 7, 10)
    &gt;&gt;&gt; decoded = sent_char_vectorizer.vectors_to_texts(vectors)
    &gt;&gt;&gt; print(decoded[0][1][:2])  # 1st text, 2d sentence, 2 words
    [[&#39;i&#39;, &#39;n&#39;], [&#39;v&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;, &#39;i&#39;, &#39;b&#39;, &#39;u&#39;, &#39;l&#39;, &#39;u&#39;, &#39;m&#39;]]
    &gt;&gt;&gt; decoded_words = sent_char_vectorizer.vectors_to_texts(vectors, True)
    &gt;&gt;&gt; print(decoded_words[0][1][:2])  # 1st text, 2d sentence, 2 words
    [&#39;in&#39;, &#39;vestibulum&#39;]
    &#34;&#34;&#34;

    def __init__(self,
                 sent_tokenize=None,
                 word_tokenize=None,
                 characters=None,
                 num_chars=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        sent_tokenize : callable or None, default None
            If set, then the function should return a list of sentences
            for a given text. When `sent_tokenize` is set to None, we assume
            that the texts are already tokenized. So each text is expected to
            be a list of sentences.

        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        characters : str, list or None, default None
            If set then only the characters defined will be in the vectrorizer&#39;s
            vocabulary. Otherwise, the characters found in the texts during
            `fit_on_texts` method will be used.

        num_chars : int or None, default None
            The maximum number of characters in the vocabulary of the
            vectorizer. If is set to None, then all the characters found using
            `fit_on_texts` method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define if the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.

        Attributes
        ----------
        num_tokens : int
            The number of tokens (characters) in the vocabulary.

        num_chars : int
            Alias for the `num_tokens` attribute.

        sents_stats : dict
            A dictionary with statistics about the sentences of the texts.
            (min/max/std/mean/median/percentiles of sentences among texts).

        words_stats : dict
            A dictionary with statistics about the tokens (words) of the texts
            (min/max/std/mean/median/percentiles of words among texts).

        chars_stats : dict
            A dictionary with statistics about the characters of the words.
            (min/max/std/mean/median/percentiles of characters among words).
        &#34;&#34;&#34;
        super().__init__(word_tokenize, characters, num_chars, filters, lower,
                         oov_token, verbose)
        self.sent_tokenize = sent_tokenize
        self.sents_stats = None
        self.words_stats = None
        self.chars_stats = None

        self.logger = logging.getLogger(self.__class__.__name__)

    def _pad_vectors(self,
                     sequences,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        if len(shape) != 3:
            raise ValueError(&#39;`shape` should be a tuple with three values.&#39;)
        # doc, sent, word, character
        max_sentences, max_words, max_characters = shape
        vectors = np.full(
            shape=(len(sequences), max_sentences, max_words, max_characters),
            fill_value=pad_value,
            dtype=int)

        self.logger.info(f&#39;Reshaping vectors to shape {shape}.&#39;)
        progbar = Progbar(len(sequences), verbose=self.verbose)
        for i in range(len(sequences)):
            num_sentences = len(sequences[i])
            words_chars_vector = np.full(
                shape=(num_sentences, max_words, max_characters),
                fill_value=pad_value,
                dtype=int)
            for j in range(num_sentences):
                chars_vector = self._pad_sequences(
                    sequences[i][j],
                    max_characters,
                    padding=padding,
                    truncating=truncating,
                    pad_value=pad_value)
                words_chars_vector[j] = self._reshape(
                    chars_vector, max_words, padding, truncating, pad_value)
            # Put the words_chars_vector into the document vector.
            vectors[i] = self._reshape(words_chars_vector, max_sentences,
                                       padding, truncating, pad_value)
            progbar.update(i)
        progbar.update(len(sequences))  # Finalize

        return vectors

    def texts_to_vectors(self,
                         texts,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;pre&#39;,
                         pad_value=None):
        if shape is not None:
            if len(shape) != 3:
                raise ValueError(
                    f&#39;The `shape` should be of rank 3 defining the&#39;
                    f&#39;maximum number of sentences per text, &#39;
                    f&#39;maximum words per sentence and the maximum &#39;
                    f&#39;characters per word. Found a shape &#39;
                    f&#39;with rank {len(shape)}.&#39;)
        if pad_value is None:
            pad_value = self.token2id[&#39;_PAD_&#39;]
        _texts = []
        self.logger.info(&#39;Converting texts to vectors.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        for text in texts:
            _text = []
            if self.sent_tokenize is None:
                # We assume that `texts` is a document already tokenized.
                # So, each `text` is a &#34;list&#34; of one sentence.
                if not isinstance(text, list):
                    raise ValueError(
                        &#39;For a sentence tokenized list of texts, &#39;
                        &#39;each text should be a list of sentences.&#39;)
                sentences = text
            else:
                sentences = self.sent_tokenize(text)
            for sentence in sentences:
                words = self._tokenize_text(sentence)
                if len(words) == 0:
                    _words = [[0]]  # The list of characters in a word.
                else:
                    _words = list(self._tokens_to_chars(words))
                _text.append(_words)
            _texts.append(_text)
            progbar.update(len(_texts))

        # Calculate document length in sentences, sentence length in words
        # and words len in characters.
        docs_len = [len(sents) for sents in _texts]
        sents_len = [len(words) for sents in _texts for words in sents]
        words_len = [
            len(chars) for sents in _texts for words in sents
            for chars in words
        ]

        self.sents_stats = calc_stats(docs_len)
        self.words_stats = calc_stats(sents_len)
        self.chars_stats = calc_stats(words_len)

        if shape is None:
            max_sentences = self.sents_stats[&#39;max&#39;]
            max_words = self.words_stats[&#39;max&#39;]
            max_characters = self.chars_stats[&#39;max&#39;]
            shape = (max_sentences, max_words, max_characters)
        vectors = self._pad_vectors(_texts, shape, padding, truncating,
                                    pad_value)
        return vectors

    def vectors_to_texts(self, vectors, as_words=False):
        texts = super().vectors_to_texts(vectors)
        if as_words:
            texts = [[[&#39;&#39;.join(chars) for chars in words] for words in sents]
                     for sents in texts]
        return texts

    def stats(self):
        return self.sents_stats, self.words_stats, self.chars_stats

    def __str__(self):
        if self.token2id is None:
            msg = &#39;SentCharVectorizer(Vocab Size: 0)&#39;
        else:
            msg = &#39;SentCharVectorizer(Vocab Size: {}, Max Words: {}, &#39; \
                  &#39;Max Characters: {})&#39;.format(
                len(self.token2id), self.words_stats[&#39;max&#39;],
                self.chars_stats[&#39;max&#39;])
        return msg

    def __repr__(self):
        return self.__str__()


class SentWordVectorizer(WordVectorizer):
    &#34;&#34;&#34;
    Convert a list of texts to an array with shape `(num_of_texts,
    max_sentences, max_words)`.

    Examples
    --------
    &gt;&gt;&gt; sent_word_vectorizer = SentWordVectorizer(verbose=0)
    &gt;&gt;&gt; # Two documents. The fists with two sentences and the second with one.
    &gt;&gt;&gt; # The 1st document is already tokenized on sentences. Alternately, you
    &gt;&gt;&gt; # may pass a sent_tokenizer callable.
    &gt;&gt;&gt; texts = [[&#39;Phasellus fermentum tellus sodales varius.&#39;, \
    &#39;In vestibulum erat nec nulla porttitor dignissim.&#39;], \
    [&#39;Nam accumsan velit vel ligula convallis.&#39;] \
    ]
    &gt;&gt;&gt; sent_word_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; vectors = sent_word_vectorizer.texts_to_vectors(texts)
    &gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_sentences, max_words)
    (2, 2, 7)
    &gt;&gt;&gt; decoded = sent_word_vectorizer.vectors_to_texts(vectors)
    &gt;&gt;&gt; print(decoded[0][1][:2])  # 1st text, 2d sentence, 2 words
    [&#39;in&#39;, &#39;vestibulum&#39;]
    &#34;&#34;&#34;

    def __init__(self,
                 sent_tokenize=None,
                 word_tokenize=None,
                 num_words=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        sent_tokenize : callable or None, default None
            If set, then the function should return a list of sentences
            for a given text. When `sent_tokenize` is set to None, we assume
            that the texts are already tokenized. So each text is expected to
            be a list of sentences.

        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        num_words : int or None, default None
            The maximum number of characters in the vocabulary of the
            vectorizer. If is set to None, then all the characters found using
            `fit_on_texts` method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define if the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.

        Attributes
        ----------
        num_tokens : int
            The number of tokens (characters) in the vocabulary.

        num_words : int
            Alias for the `num_tokens` attribute.

        sents_stats : dict
            A dictionary with statistics about the sentences of the texts.
            (min/max/std/mean/median/percentiles of sentences among texts).

        words_stats : dict
            A dictionary with statistics about the tokens (words) of the texts
            (min/max/std/mean/median/percentiles of words among texts).

        chars_stats : dict
            A dictionary with statistics about the characters of the words.
            (min/max/std/mean/median/percentiles of characters among words).
        &#34;&#34;&#34;
        super(SentWordVectorizer, self).__init__(
            word_tokenize, num_words, filters, lower, oov_token, verbose)
        self.sent_tokenize = sent_tokenize

        self.sents_stats = dict()
        self.words_stats = dict()
        self.chars_stats = dict()

    def _pad_vectors(self,
                     sequences,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        max_sentences, max_words = shape
        vectors = np.full(
            shape=(len(sequences), max_sentences, max_words),
            fill_value=self.token2id[&#39;_PAD_&#39;])
        self.logger.info(f&#39;Reshaping vectors to shape {shape}.&#39;)
        progbar = Progbar(len(sequences), verbose=self.verbose)
        for i in range(len(sequences)):
            words_vector = self._pad_sequences(
                sequences[i],
                max_words,
                padding=padding,
                truncating=truncating,
                pad_value=pad_value)
            vectors[i] = self._reshape(
                words_vector,
                num_rows=max_sentences,
                padding=padding,
                truncating=truncating,
                pad_value=pad_value)
            progbar.update(i)
        progbar.update(len(sequences))  # Finalize

        return vectors

    def texts_to_vectors(self,
                         texts,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;pre&#39;,
                         pad_value=None):
        if shape is not None:
            if len(shape) != 2:
                raise ValueError(
                    f&#39;The `shape` should be of rank 2 defining the&#39;
                    f&#39;maximum sentences per text and the maximum &#39;
                    f&#39;words per sentence. Found a shape &#39;
                    f&#39;with rank {len(shape)}.&#39;)
        if pad_value is None:
            pad_value = self.token2id[&#39;_PAD_&#39;]
        _texts = []
        self.logger.info(&#39;Converting texts to vectors.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        for text in texts:
            _text = []
            if self.sent_tokenize is None:
                # We assume that `texts` is a document already tokenized.
                # So, each `text` is a &#34;list&#34; of one sentence.
                if not isinstance(text, list):
                    raise ValueError(
                        &#39;For a sentence tokenized list of texts, &#39;
                        &#39;each text should be a list of sentences.&#39;)
                sentences = text
            else:
                sentences = self.sent_tokenize(text)
            for sentence in sentences:
                words = self._tokenize_text(sentence)
                _words = []
                for word in words:
                    if word in self.token2id:
                        _words.append(self.token2id[word])
                    else:
                        if self.oov_token is not None:
                            _words.append(self.token2id[self.oov_token])
                _text.append(_words)
            _texts.append(_text)
            progbar.update(len(_texts))

        docs_len = [len(doc) for doc in _texts]
        sents_len = [len(sent) for doc in _texts for sent in doc]
        # Words are numbers. To calculate length we get the real word back
        # using self.id2word
        words_len = [
            len(self.id2token[word]) for doc in _texts for sent in doc
            for word in sent
        ]
        self.sents_stats = calc_stats(docs_len)
        self.words_stats = calc_stats(sents_len)
        self.chars_stats = calc_stats(words_len)

        if shape is None:
            shape = self.sents_stats[&#39;max&#39;], self.words_stats[&#39;max&#39;]

        vectors = self._pad_vectors(
            _texts,
            shape,
            padding=padding,
            truncating=truncating,
            pad_value=pad_value)
        return vectors

    def stats(self):
        return self.sents_stats, self.words_stats, self.chars_stats

    def __str__(self):
        if self.token2id is None:
            msg = &#39;SentWordVectorizer(Vocab Size: 0)&#39;
        else:
            msg = &#39;SentWordVectorizer(Vocab Size: {}, &#39; \
                  &#39;Max Characters: {})&#39;.format(
                len(self.token2id), self.words_stats[&#39;max&#39;],
                self.chars_stats[&#39;max&#39;])
        return msg

    def __repr__(self):
        return self.__str__()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="keras_nlp.preprocessing.text.calc_stats"><code class="name flex">
<span>def <span class="ident">calc_stats</span></span>(<span>array)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate basic statistics for a list of numbers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>array</code></strong> :&ensp;<code>list</code>, <code>1d</code> <code>array</code>-<code>like</code></dt>
<dd>A list of numbers.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>The dictionary has keys: min, max, median, mean, std, 25-percentile,
50-percentile, 75-percentile.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_stats(array):
    &#34;&#34;&#34;
    Calculate basic statistics for a list of numbers.

    Parameters
    ----------
    array : list, 1d array-like
        A list of numbers.

    Returns
    -------
    dict
        The dictionary has keys: min, max, median, mean, std, 25-percentile,
        50-percentile, 75-percentile.

    &#34;&#34;&#34;
    array = np.asarray(array)
    stats = {
        &#39;min&#39;: np.amin(array),
        &#39;max&#39;: np.amax(array),
        &#39;median&#39;: np.median(array),
        &#39;mean&#39;: np.mean(array),
        &#39;std&#39;: np.std(array),
        &#39;25-percentile&#39;: np.percentile(array, 25),
        &#39;50-percentile&#39;: np.percentile(array, 50),
        &#39;75-percentile&#39;: np.percentile(array, 75)
    }
    return stats</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="keras_nlp.preprocessing.text.CharVectorizer"><code class="flex name class">
<span>class <span class="ident">CharVectorizer</span></span>
<span>(</span><span>word_tokenize=None, characters=None, num_chars=None, filters=&#x27;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#x27;, lower=True, oov_token=None, verbose=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Convert a list of texts to an array of vectors with shape <code>(num_of_texts,
max_words, max_characters)</code>.</p>
<p>For each text (document) keep a maximum number of words. Each word is
represented with a list of <code>max_characters</code> ids.</p>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; char_vectorizer = CharVectorizer(oov_token='?',     characters='abcdefghijklmnopqrstuvwxyz', verbose=0)
&gt;&gt;&gt; texts = ['Phasellus fermentum tellus eget libero sodales varius.',
... 'In vestibulum erat nec nulla porttitor dignissim.']
&gt;&gt;&gt; # In case `characters` are set, fit_on_texts it's secondary.
&gt;&gt;&gt; char_vectorizer.fit_on_texts(texts)
&gt;&gt;&gt; docs = ['Nam accumsan velit vel ligula convallis cursus.',
... 'Nulla porttitor felis risus, vitae facilisis massa consectetur id.']
&gt;&gt;&gt; vectors = char_vectorizer.texts_to_vectors(docs)
&gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_words, max_characters)
(2, 9, 11)
&gt;&gt;&gt; decoded = char_vectorizer.vectors_to_texts(vectors)
&gt;&gt;&gt; print(decoded[0][:2])  # First 2 words of the 1st doc in docs.
[['n', 'a', 'm'], ['a', 'c', 'c', 'u', 'm', 's', 'a', 'n']]
&gt;&gt;&gt; decoded_words = char_vectorizer.vectors_to_texts(vectors, True)
&gt;&gt;&gt; print(decoded_words[0][:2])  # First 2 words of the 1st doc in docs.
['nam', 'accumsan']
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>word_tokenize</code></strong> :&ensp;<code>callable</code> or <code>None</code>, default <code>None</code></dt>
<dd>If set, then the function should return a list of tokens (words)
for a given text. When <code>word_tokenize</code> is set to None,
the <code>keras_nlp.segment.word_tokenize()</code> method is used that split
text on white spaces, tabs and new line characters.</dd>
<dt><strong><code>characters</code></strong> :&ensp;<code>str</code>, <code>list</code> or <code>None</code>, default <code>None</code></dt>
<dd>If set then only the characters defined will be in the vectrorizer's
vocabulary. Otherwise, the characters found in the texts during
<code>fit_on_texts</code> method will be used.</dd>
<dt><strong><code>num_chars</code></strong> :&ensp;<code>int</code> or <code>None</code>, default <code>None</code></dt>
<dd>The maximum number of characters in the vocabulary of the
vectorizer. If is set to None, then all the characters found using
<code>fit_on_texts</code> method will be in the vocabulary.</dd>
<dt><strong><code>filters</code></strong> :&ensp;<code>str</code></dt>
<dd>Defines the characters that will be removed (replaced with a space
character) from the texts.</dd>
<dt><strong><code>lower</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>Define if the text will be lower cased before processing.</dd>
<dt><strong><code>oov_token</code></strong> :&ensp;<code>str</code>, <code>None</code>, default <code>None</code></dt>
<dd>If set to a string, then the vocabulary will have an entry that
represents an out of vocabulary token.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code> <code>in</code> [<code>0</code>, <code>2</code>], default <code>1</code></dt>
<dd>The verbosity of the output during the vectorization methods.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>num_tokens</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of tokens (characters) in the vocabulary.</dd>
<dt><strong><code>num_chars</code></strong> :&ensp;<code>int</code></dt>
<dd>Alias for the <code>num_tokens</code> attribute.</dd>
<dt><strong><code>words_stats</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with statistics about the tokens (words) of the texts
(min/max/std/mean/median/percentiles of words among texts).</dd>
<dt><strong><code>chars_stats</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with statistics about the characters of the words.
(min/max/std/mean/median/percentiles of characters among words).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CharVectorizer(Vectorizer):
    &#34;&#34;&#34;
    Convert a list of texts to an array of vectors with shape `(num_of_texts,
    max_words, max_characters)`.

    For each text (document) keep a maximum number of words. Each word is
    represented with a list of `max_characters` ids.

    Examples
    --------
    &gt;&gt;&gt; char_vectorizer = CharVectorizer(oov_token=&#39;?&#39;, \
    characters=&#39;abcdefghijklmnopqrstuvwxyz&#39;, verbose=0)
    &gt;&gt;&gt; texts = [&#39;Phasellus fermentum tellus eget libero sodales varius.&#39;,
    ... &#39;In vestibulum erat nec nulla porttitor dignissim.&#39;]
    &gt;&gt;&gt; # In case `characters` are set, fit_on_texts it&#39;s secondary.
    &gt;&gt;&gt; char_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; docs = [&#39;Nam accumsan velit vel ligula convallis cursus.&#39;,
    ... &#39;Nulla porttitor felis risus, vitae facilisis massa consectetur id.&#39;]
    &gt;&gt;&gt; vectors = char_vectorizer.texts_to_vectors(docs)
    &gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_words, max_characters)
    (2, 9, 11)
    &gt;&gt;&gt; decoded = char_vectorizer.vectors_to_texts(vectors)
    &gt;&gt;&gt; print(decoded[0][:2])  # First 2 words of the 1st doc in docs.
    [[&#39;n&#39;, &#39;a&#39;, &#39;m&#39;], [&#39;a&#39;, &#39;c&#39;, &#39;c&#39;, &#39;u&#39;, &#39;m&#39;, &#39;s&#39;, &#39;a&#39;, &#39;n&#39;]]
    &gt;&gt;&gt; decoded_words = char_vectorizer.vectors_to_texts(vectors, True)
    &gt;&gt;&gt; print(decoded_words[0][:2])  # First 2 words of the 1st doc in docs.
    [&#39;nam&#39;, &#39;accumsan&#39;]
    &#34;&#34;&#34;

    def __init__(self,
                 word_tokenize=None,
                 characters=None,
                 num_chars=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        characters : str, list or None, default None
            If set then only the characters defined will be in the vectrorizer&#39;s
            vocabulary. Otherwise, the characters found in the texts during
            `fit_on_texts` method will be used.

        num_chars : int or None, default None
            The maximum number of characters in the vocabulary of the
            vectorizer. If is set to None, then all the characters found using
            `fit_on_texts` method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define if the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.

        Attributes
        ----------
        num_tokens : int
            The number of tokens (characters) in the vocabulary.
        num_chars : int
            Alias for the `num_tokens` attribute.
        words_stats : dict
            A dictionary with statistics about the tokens (words) of the texts
            (min/max/std/mean/median/percentiles of words among texts).
        chars_stats : dict
            A dictionary with statistics about the characters of the words.
            (min/max/std/mean/median/percentiles of characters among words).
        &#34;&#34;&#34;
        super().__init__(word_tokenize, num_chars, filters, lower, oov_token,
                         verbose)
        self.num_chars = num_chars

        if characters is None:
            self.characters = None
        else:
            self.characters = list(characters)
            self.token_counts.update(self.characters)

        self.words_stats = dict()
        self.chars_stats = dict()

        self.logger = logging.getLogger(self.__class__.__name__)

    def fit_on_texts(self, texts):
        self._init_vocab()
        self.logger.info(&#39;Creating vocabulary.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        # Text length counting words and word length counting characters.
        texts_len, words_len = [], []
        for i, text in enumerate(texts):
            tokens = self._tokenize_text(text)
            texts_len.append(len(tokens))
            words_len.extend([len(token) for token in tokens])
            if self.characters is None:
                chars = list(&#39;&#39;.join(tokens))
                self.token_counts.update(chars)
            progbar.update(i)
        progbar.update(len(texts))  # Finalize

        self._bag_of_tokens()
        self.num_chars = self.num_tokens

        self.words_stats = calc_stats(texts_len)
        self.chars_stats = calc_stats(words_len)

    def _pad_vectors(self,
                     sequences,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        max_words, max_characters = shape
        vectors = np.full(
            shape=(len(sequences), max_words, max_characters),
            fill_value=pad_value,
            dtype=int)
        self.logger.info(f&#39;Reshaping vectors to shape {shape}.&#39;)
        progbar = Progbar(len(sequences), verbose=self.verbose)
        for i, doc in enumerate(sequences):
            chars_vector = self._pad_sequences(
                doc,
                max_characters,
                padding=padding,
                truncating=truncating,
                pad_value=pad_value)
            vectors[i] = self._reshape(chars_vector, max_words, padding,
                                       truncating, pad_value)
            progbar.update(i)
        progbar.update(len(sequences))  # Finalize
        return vectors

    def texts_to_vectors(self,
                         texts: list,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;pre&#39;,
                         pad_value=None):
        if shape is not None:
            if len(shape) != 2:
                raise ValueError(
                    f&#39;The `shape` should be of rank 2 defining the&#39;
                    f&#39;maximum words per text and the maximum &#39;
                    f&#39;characters per word. Found a shape &#39;
                    f&#39;with rank {len(shape)}.&#39;)
        if pad_value is None:
            pad_value = self.token2id[&#39;_PAD_&#39;]
        _texts = []
        self.logger.info(&#39;Converting texts to vectors.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        for text in texts:
            text = self._apply_filters(text)
            words = self.word_tokenize(text)
            # In some rare cases, all the characters of a word have been
            # removed because of the filter characters. So the length of the
            # word is zero.
            if len(words) == 0:
                _words = [[0]]  # The list of characters in a word.
            else:
                _words = list(self._tokens_to_chars(words))
            _texts.append(_words)
            progbar.update(len(_texts))

        if shape is None:
            max_words = self.words_stats[&#39;max&#39;]
            max_characters = self.chars_stats[&#39;max&#39;]
            shape = (max_words, max_characters)
        vectors = self._pad_vectors(_texts, shape, padding, truncating,
                                    pad_value)
        return vectors

    def vectors_to_texts(self, vectors, as_words=False):
        texts = super().vectors_to_texts(vectors)
        if as_words:
            texts = [[&#39;&#39;.join(chars) for chars in words] for words in texts]
        return texts

    def stats(self):
        return self.words_stats, self.chars_stats

    def __str__(self):
        if self.token2id is None:
            msg = &#39;CharVectorizer(Vocab Size: 0)&#39;
        else:
            msg = &#39;CharVectorizer(Vocab Size: {}, Max Words: {}, &#39; \
                  &#39;Max Characters: {})&#39;.format(
                len(self.token2id), self.words_stats[&#39;max&#39;],
                self.chars_stats[&#39;max&#39;])
        return msg

    def __repr__(self):
        return self.__str__()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="keras_nlp.preprocessing.text.Vectorizer" href="#keras_nlp.preprocessing.text.Vectorizer">Vectorizer</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="keras_nlp.preprocessing.text.SentCharVectorizer" href="#keras_nlp.preprocessing.text.SentCharVectorizer">SentCharVectorizer</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="keras_nlp.preprocessing.text.Vectorizer" href="#keras_nlp.preprocessing.text.Vectorizer">Vectorizer</a></b></code>:
<ul class="hlist">
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.fit_on_texts" href="#keras_nlp.preprocessing.text.Vectorizer.fit_on_texts">fit_on_texts</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.stats" href="#keras_nlp.preprocessing.text.Vectorizer.stats">stats</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.texts_to_vectors" href="#keras_nlp.preprocessing.text.Vectorizer.texts_to_vectors">texts_to_vectors</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.vectors_to_texts" href="#keras_nlp.preprocessing.text.Vectorizer.vectors_to_texts">vectors_to_texts</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="keras_nlp.preprocessing.text.SentCharVectorizer"><code class="flex name class">
<span>class <span class="ident">SentCharVectorizer</span></span>
<span>(</span><span>sent_tokenize=None, word_tokenize=None, characters=None, num_chars=None, filters=&#x27;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#x27;, lower=True, oov_token=None, verbose=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Convert a list of texts to an array of vectors with shape <code>(num_of_texts,
max_sentences, max_words, max_characters)</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; sent_char_vectorizer = SentCharVectorizer(verbose=0)
&gt;&gt;&gt; # Two documents. The fists with two sentences and the second with one.
&gt;&gt;&gt; # The 1st document is already tokenized on sentences. Alternately, you
&gt;&gt;&gt; # may pass a sent_tokenizer callable.
&gt;&gt;&gt; texts = [['Phasellus fermentum tellus eget libero sodales varius.',
... 'In vestibulum erat nec nulla porttitor dignissim.'],
... ['Nam accumsan velit vel ligula convallis cursus.']]
&gt;&gt;&gt; sent_char_vectorizer.fit_on_texts(texts)
&gt;&gt;&gt; vectors = sent_char_vectorizer.texts_to_vectors(texts)
&gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_sentences, max_words, max_characters)
(2, 2, 7, 10)
&gt;&gt;&gt; decoded = sent_char_vectorizer.vectors_to_texts(vectors)
&gt;&gt;&gt; print(decoded[0][1][:2])  # 1st text, 2d sentence, 2 words
[['i', 'n'], ['v', 'e', 's', 't', 'i', 'b', 'u', 'l', 'u', 'm']]
&gt;&gt;&gt; decoded_words = sent_char_vectorizer.vectors_to_texts(vectors, True)
&gt;&gt;&gt; print(decoded_words[0][1][:2])  # 1st text, 2d sentence, 2 words
['in', 'vestibulum']
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sent_tokenize</code></strong> :&ensp;<code>callable</code> or <code>None</code>, default <code>None</code></dt>
<dd>If set, then the function should return a list of sentences
for a given text. When <code>sent_tokenize</code> is set to None, we assume
that the texts are already tokenized. So each text is expected to
be a list of sentences.</dd>
<dt><strong><code>word_tokenize</code></strong> :&ensp;<code>callable</code> or <code>None</code>, default <code>None</code></dt>
<dd>If set, then the function should return a list of tokens (words)
for a given text. When <code>word_tokenize</code> is set to None,
the <code>keras_nlp.segment.word_tokenize()</code> method is used that split
text on white spaces, tabs and new line characters.</dd>
<dt><strong><code>characters</code></strong> :&ensp;<code>str</code>, <code>list</code> or <code>None</code>, default <code>None</code></dt>
<dd>If set then only the characters defined will be in the vectrorizer's
vocabulary. Otherwise, the characters found in the texts during
<code>fit_on_texts</code> method will be used.</dd>
<dt><strong><code>num_chars</code></strong> :&ensp;<code>int</code> or <code>None</code>, default <code>None</code></dt>
<dd>The maximum number of characters in the vocabulary of the
vectorizer. If is set to None, then all the characters found using
<code>fit_on_texts</code> method will be in the vocabulary.</dd>
<dt><strong><code>filters</code></strong> :&ensp;<code>str</code></dt>
<dd>Defines the characters that will be removed (replaced with a space
character) from the texts.</dd>
<dt><strong><code>lower</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>Define if the text will be lower cased before processing.</dd>
<dt><strong><code>oov_token</code></strong> :&ensp;<code>str</code>, <code>None</code>, default <code>None</code></dt>
<dd>If set to a string, then the vocabulary will have an entry that
represents an out of vocabulary token.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code> <code>in</code> [<code>0</code>, <code>2</code>], default <code>1</code></dt>
<dd>The verbosity of the output during the vectorization methods.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>num_tokens</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of tokens (characters) in the vocabulary.</dd>
<dt><strong><code>num_chars</code></strong> :&ensp;<code>int</code></dt>
<dd>Alias for the <code>num_tokens</code> attribute.</dd>
<dt><strong><code>sents_stats</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with statistics about the sentences of the texts.
(min/max/std/mean/median/percentiles of sentences among texts).</dd>
<dt><strong><code>words_stats</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with statistics about the tokens (words) of the texts
(min/max/std/mean/median/percentiles of words among texts).</dd>
<dt><strong><code>chars_stats</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with statistics about the characters of the words.
(min/max/std/mean/median/percentiles of characters among words).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SentCharVectorizer(CharVectorizer):
    &#34;&#34;&#34;
    Convert a list of texts to an array of vectors with shape `(num_of_texts,
    max_sentences, max_words, max_characters)`.

    Examples
    --------
    &gt;&gt;&gt; sent_char_vectorizer = SentCharVectorizer(verbose=0)
    &gt;&gt;&gt; # Two documents. The fists with two sentences and the second with one.
    &gt;&gt;&gt; # The 1st document is already tokenized on sentences. Alternately, you
    &gt;&gt;&gt; # may pass a sent_tokenizer callable.
    &gt;&gt;&gt; texts = [[&#39;Phasellus fermentum tellus eget libero sodales varius.&#39;,
    ... &#39;In vestibulum erat nec nulla porttitor dignissim.&#39;],
    ... [&#39;Nam accumsan velit vel ligula convallis cursus.&#39;]]
    &gt;&gt;&gt; sent_char_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; vectors = sent_char_vectorizer.texts_to_vectors(texts)
    &gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_sentences, max_words, max_characters)
    (2, 2, 7, 10)
    &gt;&gt;&gt; decoded = sent_char_vectorizer.vectors_to_texts(vectors)
    &gt;&gt;&gt; print(decoded[0][1][:2])  # 1st text, 2d sentence, 2 words
    [[&#39;i&#39;, &#39;n&#39;], [&#39;v&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;, &#39;i&#39;, &#39;b&#39;, &#39;u&#39;, &#39;l&#39;, &#39;u&#39;, &#39;m&#39;]]
    &gt;&gt;&gt; decoded_words = sent_char_vectorizer.vectors_to_texts(vectors, True)
    &gt;&gt;&gt; print(decoded_words[0][1][:2])  # 1st text, 2d sentence, 2 words
    [&#39;in&#39;, &#39;vestibulum&#39;]
    &#34;&#34;&#34;

    def __init__(self,
                 sent_tokenize=None,
                 word_tokenize=None,
                 characters=None,
                 num_chars=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        sent_tokenize : callable or None, default None
            If set, then the function should return a list of sentences
            for a given text. When `sent_tokenize` is set to None, we assume
            that the texts are already tokenized. So each text is expected to
            be a list of sentences.

        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        characters : str, list or None, default None
            If set then only the characters defined will be in the vectrorizer&#39;s
            vocabulary. Otherwise, the characters found in the texts during
            `fit_on_texts` method will be used.

        num_chars : int or None, default None
            The maximum number of characters in the vocabulary of the
            vectorizer. If is set to None, then all the characters found using
            `fit_on_texts` method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define if the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.

        Attributes
        ----------
        num_tokens : int
            The number of tokens (characters) in the vocabulary.

        num_chars : int
            Alias for the `num_tokens` attribute.

        sents_stats : dict
            A dictionary with statistics about the sentences of the texts.
            (min/max/std/mean/median/percentiles of sentences among texts).

        words_stats : dict
            A dictionary with statistics about the tokens (words) of the texts
            (min/max/std/mean/median/percentiles of words among texts).

        chars_stats : dict
            A dictionary with statistics about the characters of the words.
            (min/max/std/mean/median/percentiles of characters among words).
        &#34;&#34;&#34;
        super().__init__(word_tokenize, characters, num_chars, filters, lower,
                         oov_token, verbose)
        self.sent_tokenize = sent_tokenize
        self.sents_stats = None
        self.words_stats = None
        self.chars_stats = None

        self.logger = logging.getLogger(self.__class__.__name__)

    def _pad_vectors(self,
                     sequences,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        if len(shape) != 3:
            raise ValueError(&#39;`shape` should be a tuple with three values.&#39;)
        # doc, sent, word, character
        max_sentences, max_words, max_characters = shape
        vectors = np.full(
            shape=(len(sequences), max_sentences, max_words, max_characters),
            fill_value=pad_value,
            dtype=int)

        self.logger.info(f&#39;Reshaping vectors to shape {shape}.&#39;)
        progbar = Progbar(len(sequences), verbose=self.verbose)
        for i in range(len(sequences)):
            num_sentences = len(sequences[i])
            words_chars_vector = np.full(
                shape=(num_sentences, max_words, max_characters),
                fill_value=pad_value,
                dtype=int)
            for j in range(num_sentences):
                chars_vector = self._pad_sequences(
                    sequences[i][j],
                    max_characters,
                    padding=padding,
                    truncating=truncating,
                    pad_value=pad_value)
                words_chars_vector[j] = self._reshape(
                    chars_vector, max_words, padding, truncating, pad_value)
            # Put the words_chars_vector into the document vector.
            vectors[i] = self._reshape(words_chars_vector, max_sentences,
                                       padding, truncating, pad_value)
            progbar.update(i)
        progbar.update(len(sequences))  # Finalize

        return vectors

    def texts_to_vectors(self,
                         texts,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;pre&#39;,
                         pad_value=None):
        if shape is not None:
            if len(shape) != 3:
                raise ValueError(
                    f&#39;The `shape` should be of rank 3 defining the&#39;
                    f&#39;maximum number of sentences per text, &#39;
                    f&#39;maximum words per sentence and the maximum &#39;
                    f&#39;characters per word. Found a shape &#39;
                    f&#39;with rank {len(shape)}.&#39;)
        if pad_value is None:
            pad_value = self.token2id[&#39;_PAD_&#39;]
        _texts = []
        self.logger.info(&#39;Converting texts to vectors.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        for text in texts:
            _text = []
            if self.sent_tokenize is None:
                # We assume that `texts` is a document already tokenized.
                # So, each `text` is a &#34;list&#34; of one sentence.
                if not isinstance(text, list):
                    raise ValueError(
                        &#39;For a sentence tokenized list of texts, &#39;
                        &#39;each text should be a list of sentences.&#39;)
                sentences = text
            else:
                sentences = self.sent_tokenize(text)
            for sentence in sentences:
                words = self._tokenize_text(sentence)
                if len(words) == 0:
                    _words = [[0]]  # The list of characters in a word.
                else:
                    _words = list(self._tokens_to_chars(words))
                _text.append(_words)
            _texts.append(_text)
            progbar.update(len(_texts))

        # Calculate document length in sentences, sentence length in words
        # and words len in characters.
        docs_len = [len(sents) for sents in _texts]
        sents_len = [len(words) for sents in _texts for words in sents]
        words_len = [
            len(chars) for sents in _texts for words in sents
            for chars in words
        ]

        self.sents_stats = calc_stats(docs_len)
        self.words_stats = calc_stats(sents_len)
        self.chars_stats = calc_stats(words_len)

        if shape is None:
            max_sentences = self.sents_stats[&#39;max&#39;]
            max_words = self.words_stats[&#39;max&#39;]
            max_characters = self.chars_stats[&#39;max&#39;]
            shape = (max_sentences, max_words, max_characters)
        vectors = self._pad_vectors(_texts, shape, padding, truncating,
                                    pad_value)
        return vectors

    def vectors_to_texts(self, vectors, as_words=False):
        texts = super().vectors_to_texts(vectors)
        if as_words:
            texts = [[[&#39;&#39;.join(chars) for chars in words] for words in sents]
                     for sents in texts]
        return texts

    def stats(self):
        return self.sents_stats, self.words_stats, self.chars_stats

    def __str__(self):
        if self.token2id is None:
            msg = &#39;SentCharVectorizer(Vocab Size: 0)&#39;
        else:
            msg = &#39;SentCharVectorizer(Vocab Size: {}, Max Words: {}, &#39; \
                  &#39;Max Characters: {})&#39;.format(
                len(self.token2id), self.words_stats[&#39;max&#39;],
                self.chars_stats[&#39;max&#39;])
        return msg

    def __repr__(self):
        return self.__str__()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="keras_nlp.preprocessing.text.CharVectorizer" href="#keras_nlp.preprocessing.text.CharVectorizer">CharVectorizer</a></li>
<li><a title="keras_nlp.preprocessing.text.Vectorizer" href="#keras_nlp.preprocessing.text.Vectorizer">Vectorizer</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="keras_nlp.preprocessing.text.CharVectorizer" href="#keras_nlp.preprocessing.text.CharVectorizer">CharVectorizer</a></b></code>:
<ul class="hlist">
<li><code><a title="keras_nlp.preprocessing.text.CharVectorizer.fit_on_texts" href="#keras_nlp.preprocessing.text.Vectorizer.fit_on_texts">fit_on_texts</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.CharVectorizer.stats" href="#keras_nlp.preprocessing.text.Vectorizer.stats">stats</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.CharVectorizer.texts_to_vectors" href="#keras_nlp.preprocessing.text.Vectorizer.texts_to_vectors">texts_to_vectors</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.CharVectorizer.vectors_to_texts" href="#keras_nlp.preprocessing.text.Vectorizer.vectors_to_texts">vectors_to_texts</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="keras_nlp.preprocessing.text.SentWordVectorizer"><code class="flex name class">
<span>class <span class="ident">SentWordVectorizer</span></span>
<span>(</span><span>sent_tokenize=None, word_tokenize=None, num_words=None, filters=&#x27;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#x27;, lower=True, oov_token=None, verbose=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Convert a list of texts to an array with shape <code>(num_of_texts,
max_sentences, max_words)</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; sent_word_vectorizer = SentWordVectorizer(verbose=0)
&gt;&gt;&gt; # Two documents. The fists with two sentences and the second with one.
&gt;&gt;&gt; # The 1st document is already tokenized on sentences. Alternately, you
&gt;&gt;&gt; # may pass a sent_tokenizer callable.
&gt;&gt;&gt; texts = [['Phasellus fermentum tellus sodales varius.',     'In vestibulum erat nec nulla porttitor dignissim.'],     ['Nam accumsan velit vel ligula convallis.']     ]
&gt;&gt;&gt; sent_word_vectorizer.fit_on_texts(texts)
&gt;&gt;&gt; vectors = sent_word_vectorizer.texts_to_vectors(texts)
&gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_sentences, max_words)
(2, 2, 7)
&gt;&gt;&gt; decoded = sent_word_vectorizer.vectors_to_texts(vectors)
&gt;&gt;&gt; print(decoded[0][1][:2])  # 1st text, 2d sentence, 2 words
['in', 'vestibulum']
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sent_tokenize</code></strong> :&ensp;<code>callable</code> or <code>None</code>, default <code>None</code></dt>
<dd>If set, then the function should return a list of sentences
for a given text. When <code>sent_tokenize</code> is set to None, we assume
that the texts are already tokenized. So each text is expected to
be a list of sentences.</dd>
<dt><strong><code>word_tokenize</code></strong> :&ensp;<code>callable</code> or <code>None</code>, default <code>None</code></dt>
<dd>If set, then the function should return a list of tokens (words)
for a given text. When <code>word_tokenize</code> is set to None,
the <code>keras_nlp.segment.word_tokenize()</code> method is used that split
text on white spaces, tabs and new line characters.</dd>
<dt><strong><code>num_words</code></strong> :&ensp;<code>int</code> or <code>None</code>, default <code>None</code></dt>
<dd>The maximum number of characters in the vocabulary of the
vectorizer. If is set to None, then all the characters found using
<code>fit_on_texts</code> method will be in the vocabulary.</dd>
<dt><strong><code>filters</code></strong> :&ensp;<code>str</code></dt>
<dd>Defines the characters that will be removed (replaced with a space
character) from the texts.</dd>
<dt><strong><code>lower</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>Define if the text will be lower cased before processing.</dd>
<dt><strong><code>oov_token</code></strong> :&ensp;<code>str</code>, <code>None</code>, default <code>None</code></dt>
<dd>If set to a string, then the vocabulary will have an entry that
represents an out of vocabulary token.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code> <code>in</code> [<code>0</code>, <code>2</code>], default <code>1</code></dt>
<dd>The verbosity of the output during the vectorization methods.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>num_tokens</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of tokens (characters) in the vocabulary.</dd>
<dt><strong><code>num_words</code></strong> :&ensp;<code>int</code></dt>
<dd>Alias for the <code>num_tokens</code> attribute.</dd>
<dt><strong><code>sents_stats</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with statistics about the sentences of the texts.
(min/max/std/mean/median/percentiles of sentences among texts).</dd>
<dt><strong><code>words_stats</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with statistics about the tokens (words) of the texts
(min/max/std/mean/median/percentiles of words among texts).</dd>
<dt><strong><code>chars_stats</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with statistics about the characters of the words.
(min/max/std/mean/median/percentiles of characters among words).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SentWordVectorizer(WordVectorizer):
    &#34;&#34;&#34;
    Convert a list of texts to an array with shape `(num_of_texts,
    max_sentences, max_words)`.

    Examples
    --------
    &gt;&gt;&gt; sent_word_vectorizer = SentWordVectorizer(verbose=0)
    &gt;&gt;&gt; # Two documents. The fists with two sentences and the second with one.
    &gt;&gt;&gt; # The 1st document is already tokenized on sentences. Alternately, you
    &gt;&gt;&gt; # may pass a sent_tokenizer callable.
    &gt;&gt;&gt; texts = [[&#39;Phasellus fermentum tellus sodales varius.&#39;, \
    &#39;In vestibulum erat nec nulla porttitor dignissim.&#39;], \
    [&#39;Nam accumsan velit vel ligula convallis.&#39;] \
    ]
    &gt;&gt;&gt; sent_word_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; vectors = sent_word_vectorizer.texts_to_vectors(texts)
    &gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_sentences, max_words)
    (2, 2, 7)
    &gt;&gt;&gt; decoded = sent_word_vectorizer.vectors_to_texts(vectors)
    &gt;&gt;&gt; print(decoded[0][1][:2])  # 1st text, 2d sentence, 2 words
    [&#39;in&#39;, &#39;vestibulum&#39;]
    &#34;&#34;&#34;

    def __init__(self,
                 sent_tokenize=None,
                 word_tokenize=None,
                 num_words=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        sent_tokenize : callable or None, default None
            If set, then the function should return a list of sentences
            for a given text. When `sent_tokenize` is set to None, we assume
            that the texts are already tokenized. So each text is expected to
            be a list of sentences.

        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        num_words : int or None, default None
            The maximum number of characters in the vocabulary of the
            vectorizer. If is set to None, then all the characters found using
            `fit_on_texts` method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define if the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.

        Attributes
        ----------
        num_tokens : int
            The number of tokens (characters) in the vocabulary.

        num_words : int
            Alias for the `num_tokens` attribute.

        sents_stats : dict
            A dictionary with statistics about the sentences of the texts.
            (min/max/std/mean/median/percentiles of sentences among texts).

        words_stats : dict
            A dictionary with statistics about the tokens (words) of the texts
            (min/max/std/mean/median/percentiles of words among texts).

        chars_stats : dict
            A dictionary with statistics about the characters of the words.
            (min/max/std/mean/median/percentiles of characters among words).
        &#34;&#34;&#34;
        super(SentWordVectorizer, self).__init__(
            word_tokenize, num_words, filters, lower, oov_token, verbose)
        self.sent_tokenize = sent_tokenize

        self.sents_stats = dict()
        self.words_stats = dict()
        self.chars_stats = dict()

    def _pad_vectors(self,
                     sequences,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        max_sentences, max_words = shape
        vectors = np.full(
            shape=(len(sequences), max_sentences, max_words),
            fill_value=self.token2id[&#39;_PAD_&#39;])
        self.logger.info(f&#39;Reshaping vectors to shape {shape}.&#39;)
        progbar = Progbar(len(sequences), verbose=self.verbose)
        for i in range(len(sequences)):
            words_vector = self._pad_sequences(
                sequences[i],
                max_words,
                padding=padding,
                truncating=truncating,
                pad_value=pad_value)
            vectors[i] = self._reshape(
                words_vector,
                num_rows=max_sentences,
                padding=padding,
                truncating=truncating,
                pad_value=pad_value)
            progbar.update(i)
        progbar.update(len(sequences))  # Finalize

        return vectors

    def texts_to_vectors(self,
                         texts,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;pre&#39;,
                         pad_value=None):
        if shape is not None:
            if len(shape) != 2:
                raise ValueError(
                    f&#39;The `shape` should be of rank 2 defining the&#39;
                    f&#39;maximum sentences per text and the maximum &#39;
                    f&#39;words per sentence. Found a shape &#39;
                    f&#39;with rank {len(shape)}.&#39;)
        if pad_value is None:
            pad_value = self.token2id[&#39;_PAD_&#39;]
        _texts = []
        self.logger.info(&#39;Converting texts to vectors.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        for text in texts:
            _text = []
            if self.sent_tokenize is None:
                # We assume that `texts` is a document already tokenized.
                # So, each `text` is a &#34;list&#34; of one sentence.
                if not isinstance(text, list):
                    raise ValueError(
                        &#39;For a sentence tokenized list of texts, &#39;
                        &#39;each text should be a list of sentences.&#39;)
                sentences = text
            else:
                sentences = self.sent_tokenize(text)
            for sentence in sentences:
                words = self._tokenize_text(sentence)
                _words = []
                for word in words:
                    if word in self.token2id:
                        _words.append(self.token2id[word])
                    else:
                        if self.oov_token is not None:
                            _words.append(self.token2id[self.oov_token])
                _text.append(_words)
            _texts.append(_text)
            progbar.update(len(_texts))

        docs_len = [len(doc) for doc in _texts]
        sents_len = [len(sent) for doc in _texts for sent in doc]
        # Words are numbers. To calculate length we get the real word back
        # using self.id2word
        words_len = [
            len(self.id2token[word]) for doc in _texts for sent in doc
            for word in sent
        ]
        self.sents_stats = calc_stats(docs_len)
        self.words_stats = calc_stats(sents_len)
        self.chars_stats = calc_stats(words_len)

        if shape is None:
            shape = self.sents_stats[&#39;max&#39;], self.words_stats[&#39;max&#39;]

        vectors = self._pad_vectors(
            _texts,
            shape,
            padding=padding,
            truncating=truncating,
            pad_value=pad_value)
        return vectors

    def stats(self):
        return self.sents_stats, self.words_stats, self.chars_stats

    def __str__(self):
        if self.token2id is None:
            msg = &#39;SentWordVectorizer(Vocab Size: 0)&#39;
        else:
            msg = &#39;SentWordVectorizer(Vocab Size: {}, &#39; \
                  &#39;Max Characters: {})&#39;.format(
                len(self.token2id), self.words_stats[&#39;max&#39;],
                self.chars_stats[&#39;max&#39;])
        return msg

    def __repr__(self):
        return self.__str__()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="keras_nlp.preprocessing.text.WordVectorizer" href="#keras_nlp.preprocessing.text.WordVectorizer">WordVectorizer</a></li>
<li><a title="keras_nlp.preprocessing.text.Vectorizer" href="#keras_nlp.preprocessing.text.Vectorizer">Vectorizer</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="keras_nlp.preprocessing.text.WordVectorizer" href="#keras_nlp.preprocessing.text.WordVectorizer">WordVectorizer</a></b></code>:
<ul class="hlist">
<li><code><a title="keras_nlp.preprocessing.text.WordVectorizer.fit_on_texts" href="#keras_nlp.preprocessing.text.Vectorizer.fit_on_texts">fit_on_texts</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.WordVectorizer.stats" href="#keras_nlp.preprocessing.text.Vectorizer.stats">stats</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.WordVectorizer.texts_to_vectors" href="#keras_nlp.preprocessing.text.Vectorizer.texts_to_vectors">texts_to_vectors</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.WordVectorizer.vectors_to_texts" href="#keras_nlp.preprocessing.text.Vectorizer.vectors_to_texts">vectors_to_texts</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="keras_nlp.preprocessing.text.Vectorizer"><code class="flex name class">
<span>class <span class="ident">Vectorizer</span></span>
<span>(</span><span>word_tokenize=None, num_tokens=None, filters=&#x27;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#x27;, lower=True, oov_token=None, verbose=1)</span>
</code></dt>
<dd>
<section class="desc"><p>An abstract base class for different types of Vectorizers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>word_tokenize</code></strong> :&ensp;<code>callable</code> or <code>None</code>, default <code>None</code></dt>
<dd>If set, then the function should return a list of tokens (words)
for a given text. When <code>word_tokenize</code> is set to None,
the <code>keras_nlp.segment.word_tokenize()</code> method is used that split
text on white spaces, tabs and new line characters.</dd>
<dt><strong><code>num_tokens</code></strong> :&ensp;<code>int</code> or <code>None</code>, default <code>None</code></dt>
<dd>The maximum number of tokens in the vocabulary of the vectorizer.
If is set to None, then all the tokens found using <code>fit_on_texts</code>
method will be in the vocabulary.</dd>
<dt><strong><code>filters</code></strong> :&ensp;<code>str</code></dt>
<dd>Defines the characters that will be removed (replaced with a space
character) from the texts.</dd>
<dt><strong><code>lower</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>Define it the text will be lower cased before processing.</dd>
<dt><strong><code>oov_token</code></strong> :&ensp;<code>str</code>, <code>None</code>, default <code>None</code></dt>
<dd>If set to a string, then the vocabulary will have an entry that
represents an out of vocabulary token.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code> <code>in</code> [<code>0</code>, <code>2</code>], default <code>1</code></dt>
<dd>The verbosity of the output during the vectorization methods.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Vectorizer:
    &#34;&#34;&#34;
    An abstract base class for different types of Vectorizers.
    &#34;&#34;&#34;

    def __init__(self,
                 word_tokenize=None,
                 num_tokens=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        num_tokens : int or None, default None
            The maximum number of tokens in the vocabulary of the vectorizer.
            If is set to None, then all the tokens found using `fit_on_texts`
            method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define it the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.
        &#34;&#34;&#34;
        if word_tokenize is None:
            self.word_tokenize = segment.word_tokenize
        self.num_tokens = num_tokens
        if filters is not None:
            self.filters = &#39;[&#39; + re.escape(filters) + &#39;]&#39;
        else:
            self.filters = None
        self.lower = lower
        self.oov_token = oov_token

        self.token2id = None
        self.id2token = None
        self.token_counts = Counter()

        self.verbose = verbose
        self.logger = logging.getLogger(self.__class__.__name__)

    def _apply_filters(self, text):
        &#34;&#34;&#34; Lowers and removes filters from the text. &#34;&#34;&#34;
        if self.lower:
            text = text.lower()

        if self.filters is not None:
            text = re.sub(self.filters, &#39; &#39;, text)

        return text

    def _bag_of_tokens(self):
        &#34;&#34;&#34;
        Create the `self.token2id` and `self.id2token` vocabularies.
        &#34;&#34;&#34;
        if self.token2id is None:
            raise ValueError(&#39;Please use the `fit_on_texts` method first.&#39;)
        if self.num_tokens is not None:
            tokens = self.token_counts.most_common(self.num_tokens)
        else:
            tokens = self.token_counts.most_common()

        if len(tokens) &gt; 0:
            # Tokens are pairs of (token, count). We keep only the token.
            tokens = [token[0] for token in tokens]
            for token in tokens:
                self.token2id[token] = len(self.token2id)
            self.id2token = {v: k for k, v in self.token2id.items()}
        self.num_tokens = len(self.token2id)

    def _init_vocab(self):
        self.token2id = {&#39;_PAD_&#39;: 0}
        if self.oov_token is not None:
            self.token2id[self.oov_token] = 1

    @abc.abstractmethod
    def fit_on_texts(self, texts):
        &#34;&#34;&#34;
        Creates a tokens vocabulary from the list of texts.

        Parameters
        ----------
        texts : list or list[list]
            Each list item can be a text representing a document or a list of
            texts.

        &#34;&#34;&#34;
        raise NotImplementedError()

    @abc.abstractmethod
    def _pad_vectors(self,
                     vectors,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        &#34;&#34;&#34;
        Pad vectors to 2D or 3D arrays.

        A `CharVectorizer` will output list[list] vectors where the outer list
        represent a text and the inner list the tokens. The method will
        ensure that all texts have the same number of tokens and all tokens
        have the same number of characters.
        A `SentWordVectorizer` will output list[list] vectors where the outer
        list represent a sentence and the inner list the tokens. The method will
        ensure that all texts have the same number of sentences and all
        sentences have the same number of tokens.
        A `SentCharVectorizer` will output list[list[list]] vectors. The
        outer list represents the number of sentences of a text, the 2nd
        represents the number of tokens and the 3d the number of characters
        per token.

        Parameters
        ----------
        vectors : list
            A list[list] or list[list[list]] of ids depending on the instance
            of the Vectorizer.

        shape : int, list or tuple
            The target shape of the vectors padded or truncated depending
            on the `shape`&#39;s 1st dimension and the length of `vectors`.

        padding : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            Add `pad_value`s to vectors, based on the selected method, in
            order to match the target `shape`.

        truncating : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            Delete values, based on the selected method, from `vectors` in
            order to match the target `shape`.

        pad_value : int, default 0
            In case of padding, the value to use.

        Returns
        -------
        ndarray
            An array of shape `shape`.
        &#34;&#34;&#34;
        raise NotImplementedError()

    @staticmethod
    def _pad_sequences(sequences,
                       maxlen,
                       padding=&#39;pre&#39;,
                       truncating=&#39;pre&#39;,
                       pad_value=0):
        array = np.full(shape=(len(sequences), maxlen), fill_value=pad_value)
        for idx, seq in enumerate(sequences):
            if not len(seq):
                continue
            if len(seq) &lt; maxlen:
                if padding == &#39;pre&#39;:
                    array[idx, -len(seq):] = seq
                elif padding == &#39;post&#39;:
                    array[idx, :len(seq)] = seq
                else:
                    raise ValueError(
                        f&#39;Unknown option `{padding}` for padding.&#39;)
            else:
                if truncating == &#39;pre&#39;:
                    array[idx, :] = seq[-maxlen:]
                elif truncating == &#39;post&#39;:
                    array[idx, :] = seq[:maxlen]
                else:
                    if not isinstance(truncating, (tuple, list)):
                        raise ValueError(
                            f&#39;Unknown option `{truncating}` for truncating.&#39;)
                    if len(truncating) != 2:
                        raise ValueError(
                            &#39;`truncating` is expected to be a tuple &#39;
                            &#39;with two numbers; one for the &#39;
                            &#39;percentage of text to keep from the &#39;
                            &#39;beginning and one for the end.&#39;)
                    if sum(truncating) != 1:
                        raise ValueError(&#39;`truncating` should sum to 1.&#39;)
                    pre = round(maxlen * truncating[0])
                    post = int(np.ceil(maxlen * truncating[1]))
                    array[idx, :] = seq[:pre] + seq[-post:]
        return array

    @staticmethod
    def _reshape(vectors,
                 num_rows,
                 padding=&#39;pre&#39;,
                 truncating=&#39;pre&#39;,
                 pad_value=0):
        &#34;&#34;&#34;
        Reshape `vectors` to `(num_rows, vectors.shape[1:])`.

        This function is used in order to cut or add rows to the `vectors`
        array, keep the rest dimensions.  This is usefull to make all documents
        of a collection to have the same number of (words, characters) or
        (sentences, words) in case of 2D `vectors`, or (sentences, words,
        characters) in case of 3D `vectors`.

        Parameters
        ----------
        vectors : ndarray
            A 2D or 3D array, depending on the calling instance of `Vectorizer`

        num_rows : int
            The new number or rows of the `vectors`, padded or truncated
            if needed.

        padding: str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            The padding method to use in case `num_rows &gt; sequences.shape[1]`

        truncating : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            The truncating method to use in case `num_rows &lt; sequences.shape[1]`

        pad_value: int, default 0
            The value to use in case of padding.

        Returns
        -------
        ndarray

        Raises
        ------
        ValueError
            `padding` or `truncating` values are not recognized.
        &#34;&#34;&#34;
        target_shape = [num_rows] + list(vectors.shape[1:])
        array = np.full(target_shape, fill_value=pad_value)
        if vectors.shape[0] &lt; target_shape[0]:
            if padding == &#39;pre&#39;:
                array[-vectors.shape[0]:, :] = vectors
            elif padding == &#39;post&#39;:
                array[:vectors.shape[0], :] = vectors
            else:
                raise ValueError(
                    &#39;Unknown option `{}` for padding.&#39;.format(padding))
        elif vectors.shape[0] &gt; target_shape[0]:
            if truncating == &#39;pre&#39;:
                array = vectors[-target_shape[0]:, :]
            elif truncating == &#39;post&#39;:
                array = vectors[:target_shape[0], :]
            else:
                if not isinstance(truncating, (tuple, list)):
                    raise ValueError(
                        f&#39;Unknown option `{truncating}` for truncating.&#39;)
                if len(truncating) != 2:
                    raise ValueError(&#39;`truncating` is expected to be a tuple &#39;
                                     &#39;with two numbers; one for the &#39;
                                     &#39;percentage of text to keep from the &#39;
                                     &#39;beginning and one for the end.&#39;)
                if sum(truncating) != 1:
                    raise ValueError(&#39;`truncating` should sum to 1.&#39;)
                pre = round(target_shape[0] * truncating[0])
                post = int(np.ceil(target_shape[0] * truncating[1]))
                # Mask the values between pre and post to False.
                mask = np.ones(len(vectors), dtype=bool)
                mask[range(pre, len(vectors) - post)] = False
                array = vectors[mask, ...]
        else:
            array = vectors
        return array

    def _tokenize_text(self, text):
        if isinstance(text, list):
            _text = []
            for sentence in text:
                sentence = self._apply_filters(sentence)
                words = self.word_tokenize(sentence)
                _text.append(words)
            tokenized_text = _text
        else:
            text = self._apply_filters(text)
            words = self.word_tokenize(text)
            tokenized_text = words

        return tokenized_text

    def _tokens_to_chars(self, tokens):
        &#34;&#34;&#34;
        For each token create a list of it&#39;s character ids.

        Parameters
        ----------
        tokens : list
            The list of tokens to get the character ids from.

        Yields
        ------
        list[list]
            Each list item corresponds to a token. Each token is a list of its
            character ids.
        &#34;&#34;&#34;
        for token in tokens:
            _chars = []
            for char in token:
                if char in self.token2id:
                    _chars.append(self.token2id[char])
                else:
                    if self.oov_token is not None:
                        _chars.append(self.token2id[self.oov_token])
            yield _chars

    @abc.abstractmethod
    def texts_to_vectors(self,
                         texts,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;post&#39;,
                         pad_value=None):
        &#34;&#34;&#34;
        Convert a list of texts to a 2D, 3D or 4D array.

        * A `WordVectorizer` will return a 2D array of shape `(len(texts),
        max_tokens)`.
        * A `CharVectorizer` will return a 3D array of shape `(len(texts),
        max_tokens, max_characters)`.
        * A `SentWordVectorizer` will return a 3D array of shape `(len(texts),
        max_sentences, max_tokens)`.
        * A `SentCharVectorizer` will return a 4D array of shape `(len(texts),
        max_sentences, max_tokens, max_characters)`.

        Parameters
        ----------
        texts : list
            The list of texts to convert. Each item represents a document.

        shape : tuple, list or None, default None
            * In case of `WordVectorizer` the shape defines the tuple
            `(max_tokens, )` per text.
            * In case of `CharVectorizer` the shape defines the tuple `(
            max_tokens, max_characters)`.
            * In case of `SentWordVectorizer` the shape defines the tuple `(
            max_sentences, max_tokens)`.
            * In case of `SentCharVectorizer` the shape defines the tuple `(
            max_sentences, max_tokens, max_characters)`.

        padding : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            Defines the padding method when the length of tokens/characters
            is less than `max_tokens`/`max_characters`. The filling value is
            the value of the `pad_value` parameter.

        truncating : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
            Defines the cutting method when  the length of tokens/characters
            is larger than `max_tokens/characters`.

        pad_value : int, default None
            The value to use when padding is needed. If the value is `None`
            then the `token2id[&#39;_PAD_&#39;]` value is used which by default is 0.

        Returns
        -------
        ndarray : Numpy 2D array with shape `shape`

        Raises
        ------
        ValueError : `shape` len is not 2 and `shape` is not None.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def vectors_to_texts(self, vectors):
        &#34;&#34;&#34;
        Decode `vectors` array to texts using the Vectorizer&#39;s vocabulary.

        Parameters
        ----------
        vectors : ndarray
            The shape of `vectors` may be:

            * 2D `(num_of_texts, num_of_tokens)` from `WordVectorizer`;
            * 3D `(num_of_texts, num_of_sentences, num_of_words)` from
            `SentWordVectorizer`;
            * 3D `(num_of_texts, num_of_words, num_of_characters)` from
            `CharVectorizer`;
            a 4D `(num_of_texts, num_of_sentences, num_of_words,
            num_of_characters)` from `SentCharVectorizer`.

        Returns
        -------
        list
            The list has length of `num_of_texts`.

            * In case of 2D input, the list has the words of the texts.
            * In case of 3D input, the list has a list sentences. Each item of
            the nested list is a word in case of `SentWordVectorizer`,
            or the list has a list of words. Each item of the word&#39;s list is
            the characters of the words.
            * In case of 4D input, the output is a list of texts with their list
            of sentences. For each sentence a list of tokens where each token
            is a list of its characters.
        &#34;&#34;&#34;

        def decode_vector(vector):
            text = []
            for values in vector:
                tokens = [
                    self.id2token[t] for t in values
                    if t != self.token2id[&#39;_PAD_&#39;]
                ]
                if len(tokens) &gt; 0:
                    text.append(tokens)
            return text

        if self.id2token is None:
            raise ValueError(&#39;Please use `fit_on_texts` method first.&#39;)
        self.logger.info(&#39;Converting vectors to texts.&#39;)
        texts = []
        progbar = Progbar(vectors.shape[0], verbose=self.verbose)
        if vectors.ndim == 2:
            for doc in vectors:
                # Just to be able to update progress bar.
                text = decode_vector(doc.reshape(1, -1))
                # text is a list with one item; the words of the document.
                texts.append(text[0])
                progbar.update(len(texts))
        elif vectors.ndim == 3:
            for vector in vectors:
                decoded = decode_vector(vector)
                if len(decoded) &gt; 0:  # If len==0, it is padded.
                    texts.append(decoded)
                progbar.update(len(texts))
        elif vectors.ndim == 4:
            for doc in vectors:
                text = []
                for sents in doc:
                    decoded = decode_vector(sents)
                    if len(decoded) &gt; 0:  # If len==0, it is padded.
                        text.append(decoded)
                texts.append(text)
                progbar.update(len(texts))

        return texts

    @abc.abstractmethod
    def stats(self):
        &#34;&#34;&#34;
        Get statistics about the texts the Vectorizer is fitted on.

        Returns
        -------
        dict
        &#34;&#34;&#34;
        raise NotImplementedError()

    def __getstate__(self):
        # `logger` object is not pickable. So we remove it.
        state = self.__dict__.copy()
        if &#39;logger&#39; in state:
            del state[&#39;logger&#39;]
        return state

    def __setstate__(self, state):
        # Unpickling the object, we don&#39;t have a logger instance which is
        # used in some methods. So we create a new one.
        self.__dict__ = state
        self.__dict__[&#39;logger&#39;] = logging.getLogger(self.__class__.__name__)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="keras_nlp.preprocessing.text.CharVectorizer" href="#keras_nlp.preprocessing.text.CharVectorizer">CharVectorizer</a></li>
<li><a title="keras_nlp.preprocessing.text.WordVectorizer" href="#keras_nlp.preprocessing.text.WordVectorizer">WordVectorizer</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="keras_nlp.preprocessing.text.Vectorizer.fit_on_texts"><code class="name flex">
<span>def <span class="ident">fit_on_texts</span></span>(<span>self, texts)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a tokens vocabulary from the list of texts.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>texts</code></strong> :&ensp;<code>list</code> or <code>list</code>[<code>list</code>]</dt>
<dd>Each list item can be a text representing a document or a list of
texts.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def fit_on_texts(self, texts):
    &#34;&#34;&#34;
    Creates a tokens vocabulary from the list of texts.

    Parameters
    ----------
    texts : list or list[list]
        Each list item can be a text representing a document or a list of
        texts.

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="keras_nlp.preprocessing.text.Vectorizer.stats"><code class="name flex">
<span>def <span class="ident">stats</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get statistics about the texts the Vectorizer is fitted on.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def stats(self):
    &#34;&#34;&#34;
    Get statistics about the texts the Vectorizer is fitted on.

    Returns
    -------
    dict
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="keras_nlp.preprocessing.text.Vectorizer.texts_to_vectors"><code class="name flex">
<span>def <span class="ident">texts_to_vectors</span></span>(<span>self, texts, shape=None, padding='pre', truncating='post', pad_value=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Convert a list of texts to a 2D, 3D or 4D array.</p>
<ul>
<li>A <a title="keras_nlp.preprocessing.text.WordVectorizer" href="#keras_nlp.preprocessing.text.WordVectorizer"><code>WordVectorizer</code></a> will return a 2D array of shape <code>(len(texts),
max_tokens)</code>.</li>
<li>A <a title="keras_nlp.preprocessing.text.CharVectorizer" href="#keras_nlp.preprocessing.text.CharVectorizer"><code>CharVectorizer</code></a> will return a 3D array of shape <code>(len(texts),
max_tokens, max_characters)</code>.</li>
<li>A <a title="keras_nlp.preprocessing.text.SentWordVectorizer" href="#keras_nlp.preprocessing.text.SentWordVectorizer"><code>SentWordVectorizer</code></a> will return a 3D array of shape <code>(len(texts),
max_sentences, max_tokens)</code>.</li>
<li>A <a title="keras_nlp.preprocessing.text.SentCharVectorizer" href="#keras_nlp.preprocessing.text.SentCharVectorizer"><code>SentCharVectorizer</code></a> will return a 4D array of shape <code>(len(texts),
max_sentences, max_tokens, max_characters)</code>.</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>texts</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of texts to convert. Each item represents a document.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>tuple</code>, <code>list</code> or <code>None</code>, default <code>None</code></dt>
<dd>
<ul>
<li>In case of <a title="keras_nlp.preprocessing.text.WordVectorizer" href="#keras_nlp.preprocessing.text.WordVectorizer"><code>WordVectorizer</code></a> the shape defines the tuple
<code>(max_tokens, )</code> per text.</li>
<li>In case of <a title="keras_nlp.preprocessing.text.CharVectorizer" href="#keras_nlp.preprocessing.text.CharVectorizer"><code>CharVectorizer</code></a> the shape defines the tuple <code>(
max_tokens, max_characters)</code>.</li>
<li>In case of <a title="keras_nlp.preprocessing.text.SentWordVectorizer" href="#keras_nlp.preprocessing.text.SentWordVectorizer"><code>SentWordVectorizer</code></a> the shape defines the tuple <code>(
max_sentences, max_tokens)</code>.</li>
<li>In case of <a title="keras_nlp.preprocessing.text.SentCharVectorizer" href="#keras_nlp.preprocessing.text.SentCharVectorizer"><code>SentCharVectorizer</code></a> the shape defines the tuple <code>(
max_sentences, max_tokens, max_characters)</code>.</li>
</ul>
</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>str</code>, <code>options</code> {<code>'pre'</code>, <code>'post'</code>}, default <code>'pre'</code></dt>
<dd>Defines the padding method when the length of tokens/characters
is less than <code>max_tokens</code>/<code>max_characters</code>. The filling value is
the value of the <code>pad_value</code> parameter.</dd>
<dt><strong><code>truncating</code></strong> :&ensp;<code>str</code>, <code>options</code> {<code>'pre'</code>, <code>'post'</code>}, default <code>'pre'</code></dt>
<dd>Defines the cutting method when
the length of tokens/characters
is larger than <code>max_tokens/characters</code>.</dd>
<dt><strong><code>pad_value</code></strong> :&ensp;<code>int</code>, default <code>None</code></dt>
<dd>The value to use when padding is needed. If the value is <code>None</code>
then the <code>token2id['_PAD_']</code> value is used which by default is 0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ndarray</code></strong> :&ensp;<code>Numpy</code> <code>2D</code> <code>array</code> <code>with</code> <code>shape</code> <code>shape</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="raises">Raises</h2>
<p>ValueError : <code>shape</code> len is not 2 and <code>shape</code> is not None.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def texts_to_vectors(self,
                     texts,
                     shape=None,
                     padding=&#39;pre&#39;,
                     truncating=&#39;post&#39;,
                     pad_value=None):
    &#34;&#34;&#34;
    Convert a list of texts to a 2D, 3D or 4D array.

    * A `WordVectorizer` will return a 2D array of shape `(len(texts),
    max_tokens)`.
    * A `CharVectorizer` will return a 3D array of shape `(len(texts),
    max_tokens, max_characters)`.
    * A `SentWordVectorizer` will return a 3D array of shape `(len(texts),
    max_sentences, max_tokens)`.
    * A `SentCharVectorizer` will return a 4D array of shape `(len(texts),
    max_sentences, max_tokens, max_characters)`.

    Parameters
    ----------
    texts : list
        The list of texts to convert. Each item represents a document.

    shape : tuple, list or None, default None
        * In case of `WordVectorizer` the shape defines the tuple
        `(max_tokens, )` per text.
        * In case of `CharVectorizer` the shape defines the tuple `(
        max_tokens, max_characters)`.
        * In case of `SentWordVectorizer` the shape defines the tuple `(
        max_sentences, max_tokens)`.
        * In case of `SentCharVectorizer` the shape defines the tuple `(
        max_sentences, max_tokens, max_characters)`.

    padding : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
        Defines the padding method when the length of tokens/characters
        is less than `max_tokens`/`max_characters`. The filling value is
        the value of the `pad_value` parameter.

    truncating : str, options {&#39;pre&#39;, &#39;post&#39;}, default &#39;pre&#39;
        Defines the cutting method when  the length of tokens/characters
        is larger than `max_tokens/characters`.

    pad_value : int, default None
        The value to use when padding is needed. If the value is `None`
        then the `token2id[&#39;_PAD_&#39;]` value is used which by default is 0.

    Returns
    -------
    ndarray : Numpy 2D array with shape `shape`

    Raises
    ------
    ValueError : `shape` len is not 2 and `shape` is not None.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="keras_nlp.preprocessing.text.Vectorizer.vectors_to_texts"><code class="name flex">
<span>def <span class="ident">vectors_to_texts</span></span>(<span>self, vectors)</span>
</code></dt>
<dd>
<section class="desc"><p>Decode <code>vectors</code> array to texts using the Vectorizer's vocabulary.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vectors</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>
<p>The shape of <code>vectors</code> may be:</p>
<ul>
<li>2D <code>(num_of_texts, num_of_tokens)</code> from <a title="keras_nlp.preprocessing.text.WordVectorizer" href="#keras_nlp.preprocessing.text.WordVectorizer"><code>WordVectorizer</code></a>;</li>
<li>3D <code>(num_of_texts, num_of_sentences, num_of_words)</code> from
<a title="keras_nlp.preprocessing.text.SentWordVectorizer" href="#keras_nlp.preprocessing.text.SentWordVectorizer"><code>SentWordVectorizer</code></a>;</li>
<li>3D <code>(num_of_texts, num_of_words, num_of_characters)</code> from
<a title="keras_nlp.preprocessing.text.CharVectorizer" href="#keras_nlp.preprocessing.text.CharVectorizer"><code>CharVectorizer</code></a>;
a 4D <code>(num_of_texts, num_of_sentences, num_of_words,
num_of_characters)</code> from <a title="keras_nlp.preprocessing.text.SentCharVectorizer" href="#keras_nlp.preprocessing.text.SentCharVectorizer"><code>SentCharVectorizer</code></a>.</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>
<p>The list has length of <code>num_of_texts</code>.</p>
<ul>
<li>In case of 2D input, the list has the words of the texts.</li>
<li>In case of 3D input, the list has a list sentences. Each item of
the nested list is a word in case of <a title="keras_nlp.preprocessing.text.SentWordVectorizer" href="#keras_nlp.preprocessing.text.SentWordVectorizer"><code>SentWordVectorizer</code></a>,
or the list has a list of words. Each item of the word's list is
the characters of the words.</li>
<li>In case of 4D input, the output is a list of texts with their list
of sentences. For each sentence a list of tokens where each token
is a list of its characters.</li>
</ul>
</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vectors_to_texts(self, vectors):
    &#34;&#34;&#34;
    Decode `vectors` array to texts using the Vectorizer&#39;s vocabulary.

    Parameters
    ----------
    vectors : ndarray
        The shape of `vectors` may be:

        * 2D `(num_of_texts, num_of_tokens)` from `WordVectorizer`;
        * 3D `(num_of_texts, num_of_sentences, num_of_words)` from
        `SentWordVectorizer`;
        * 3D `(num_of_texts, num_of_words, num_of_characters)` from
        `CharVectorizer`;
        a 4D `(num_of_texts, num_of_sentences, num_of_words,
        num_of_characters)` from `SentCharVectorizer`.

    Returns
    -------
    list
        The list has length of `num_of_texts`.

        * In case of 2D input, the list has the words of the texts.
        * In case of 3D input, the list has a list sentences. Each item of
        the nested list is a word in case of `SentWordVectorizer`,
        or the list has a list of words. Each item of the word&#39;s list is
        the characters of the words.
        * In case of 4D input, the output is a list of texts with their list
        of sentences. For each sentence a list of tokens where each token
        is a list of its characters.
    &#34;&#34;&#34;

    def decode_vector(vector):
        text = []
        for values in vector:
            tokens = [
                self.id2token[t] for t in values
                if t != self.token2id[&#39;_PAD_&#39;]
            ]
            if len(tokens) &gt; 0:
                text.append(tokens)
        return text

    if self.id2token is None:
        raise ValueError(&#39;Please use `fit_on_texts` method first.&#39;)
    self.logger.info(&#39;Converting vectors to texts.&#39;)
    texts = []
    progbar = Progbar(vectors.shape[0], verbose=self.verbose)
    if vectors.ndim == 2:
        for doc in vectors:
            # Just to be able to update progress bar.
            text = decode_vector(doc.reshape(1, -1))
            # text is a list with one item; the words of the document.
            texts.append(text[0])
            progbar.update(len(texts))
    elif vectors.ndim == 3:
        for vector in vectors:
            decoded = decode_vector(vector)
            if len(decoded) &gt; 0:  # If len==0, it is padded.
                texts.append(decoded)
            progbar.update(len(texts))
    elif vectors.ndim == 4:
        for doc in vectors:
            text = []
            for sents in doc:
                decoded = decode_vector(sents)
                if len(decoded) &gt; 0:  # If len==0, it is padded.
                    text.append(decoded)
            texts.append(text)
            progbar.update(len(texts))

    return texts</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="keras_nlp.preprocessing.text.WordVectorizer"><code class="flex name class">
<span>class <span class="ident">WordVectorizer</span></span>
<span>(</span><span>word_tokenize=None, num_words=None, filters=&#x27;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#x27;, lower=True, oov_token=None, verbose=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Convert a list of texts to an array of vectors with shape <code>(num_of_texts,
max_words)</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; word_vectorizer = WordVectorizer(verbose=0)
&gt;&gt;&gt; texts = ['Phasellus fermentum tellus eget libero sodales varius.',
... 'In vestibulum erat nec nulla porttitor dignissim.']
&gt;&gt;&gt; word_vectorizer.fit_on_texts(texts)
&gt;&gt;&gt; vectors = word_vectorizer.texts_to_vectors(texts)
&gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_words)
(2, 7)
&gt;&gt;&gt; decoded = word_vectorizer.vectors_to_texts(vectors)
&gt;&gt;&gt; print(decoded[0][:3])  # First 3 words of the 1st doc in docs.
['phasellus', 'fermentum', 'tellus']
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>word_tokenize</code></strong> :&ensp;<code>callable</code> or <code>None</code>, default <code>None</code></dt>
<dd>If set, then the function should return a list of tokens (words)
for a given text. When <code>word_tokenize</code> is set to None,
the <code>keras_nlp.segment.word_tokenize()</code> method is used that split
text on white spaces, tabs and new line characters.</dd>
<dt><strong><code>num_words</code></strong> :&ensp;<code>int</code> or <code>None</code>, default <code>None</code></dt>
<dd>The maximum number of words in the vocabulary of the
vectorizer. If is set to None, then all the words found using
<code>fit_on_texts</code> method will be in the vocabulary.</dd>
<dt><strong><code>filters</code></strong> :&ensp;<code>str</code></dt>
<dd>Defines the characters that will be removed (replaced with a space
character) from the texts.</dd>
<dt><strong><code>lower</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>Define if the text will be lower cased before processing.</dd>
<dt><strong><code>oov_token</code></strong> :&ensp;<code>str</code>, <code>None</code>, default <code>None</code></dt>
<dd>If set to a string, then the vocabulary will have an entry that
represents an out of vocabulary token.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code> <code>in</code> [<code>0</code>, <code>2</code>], default <code>1</code></dt>
<dd>The verbosity of the output during the vectorization methods.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>num_tokens</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of tokens (words) in the vocabulary.</dd>
<dt><strong><code>num_words</code></strong> :&ensp;<code>int</code></dt>
<dd>Alias for the <code>num_tokens</code> attribute.</dd>
<dt><strong><code>words_stats</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with statistics about the tokens (words) of the texts
(min/max/std/mean/median/percentiles of words among texts).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WordVectorizer(Vectorizer):
    &#34;&#34;&#34;
    Convert a list of texts to an array of vectors with shape `(num_of_texts,
    max_words)`.

    Examples
    --------
    &gt;&gt;&gt; word_vectorizer = WordVectorizer(verbose=0)
    &gt;&gt;&gt; texts = [&#39;Phasellus fermentum tellus eget libero sodales varius.&#39;,
    ... &#39;In vestibulum erat nec nulla porttitor dignissim.&#39;]
    &gt;&gt;&gt; word_vectorizer.fit_on_texts(texts)
    &gt;&gt;&gt; vectors = word_vectorizer.texts_to_vectors(texts)
    &gt;&gt;&gt; print(vectors.shape)  # (len(texts), max_words)
    (2, 7)
    &gt;&gt;&gt; decoded = word_vectorizer.vectors_to_texts(vectors)
    &gt;&gt;&gt; print(decoded[0][:3])  # First 3 words of the 1st doc in docs.
    [&#39;phasellus&#39;, &#39;fermentum&#39;, &#39;tellus&#39;]
    &#34;&#34;&#34;

    def __init__(self,
                 word_tokenize=None,
                 num_words=None,
                 filters=&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n&#39;,
                 lower=True,
                 oov_token=None,
                 verbose=1):
        &#34;&#34;&#34;
        Parameters
        ----------
        word_tokenize : callable or None, default None
            If set, then the function should return a list of tokens (words)
            for a given text. When `word_tokenize` is set to None,
            the `keras_nlp.segment.word_tokenize()` method is used that split
            text on white spaces, tabs and new line characters.

        num_words : int or None, default None
            The maximum number of words in the vocabulary of the
            vectorizer. If is set to None, then all the words found using
            `fit_on_texts` method will be in the vocabulary.

        filters : str
            Defines the characters that will be removed (replaced with a space
            character) from the texts.

        lower : bool, default True
            Define if the text will be lower cased before processing.

        oov_token : str, None, default None
            If set to a string, then the vocabulary will have an entry that
            represents an out of vocabulary token.

        verbose : int in [0, 2], default 1
            The verbosity of the output during the vectorization methods.

        Attributes
        ----------
        num_tokens : int
            The number of tokens (words) in the vocabulary.

        num_words : int
            Alias for the `num_tokens` attribute.

        words_stats : dict
            A dictionary with statistics about the tokens (words) of the texts
            (min/max/std/mean/median/percentiles of words among texts).
        &#34;&#34;&#34;
        super().__init__(word_tokenize, num_words, filters, lower, oov_token,
                         verbose)
        self.words_stats = dict()
        self.logger = logging.getLogger(self.__class__.__name__)

    def fit_on_texts(self, texts):
        self._init_vocab()
        self.logger.info(&#39;Creating vocabulary.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        docs_len = []
        for i, text in enumerate(texts):
            tokens = self._tokenize_text(text)
            docs_len.append(len(tokens))
            self.token_counts.update(tokens)
            progbar.update(i)
        progbar.update(len(texts))

        self._bag_of_tokens()
        self.words_stats = calc_stats(docs_len)

    def _pad_vectors(self,
                     vectors,
                     shape,
                     padding=&#39;pre&#39;,
                     truncating=&#39;pre&#39;,
                     pad_value=0):
        pass

    def texts_to_vectors(self,
                         texts: list,
                         shape=None,
                         padding=&#39;pre&#39;,
                         truncating=&#39;pre&#39;,
                         pad_value=None):
        if shape is not None:
            if len(shape) != 1:
                raise ValueError(
                    f&#39;The `shape` should be of rank 1 defining the&#39;
                    f&#39;maximum words per text. Found a shape &#39;
                    f&#39;with rank {len(shape)}.&#39;)
        if pad_value is None:
            pad_value = self.token2id[&#39;_PAD_&#39;]
        _texts = []
        self.logger.info(&#39;Converting texts to vectors.&#39;)
        progbar = Progbar(len(texts), verbose=self.verbose)
        for text in texts:
            _words = []
            text = self._apply_filters(text)
            words = self.word_tokenize(text)
            for word in words:
                if word in self.token2id:
                    _words.append(self.token2id[word])
                else:
                    if self.oov_token is not None:
                        _words.append(self.token2id[self.oov_token])
            _texts.append(_words)
            progbar.update(len(_texts))

        if shape is None:
            shape = (self.words_stats[&#39;max&#39;], )

        vectors = self._pad_sequences(
            _texts,
            shape[0],
            padding=padding,
            truncating=truncating,
            pad_value=pad_value)
        return vectors

    def stats(self):
        return self.words_stats

    def __str__(self):
        if self.token2id is None:
            msg = &#39;WordVectorizer(Vocab Size: 0)&#39;
        else:
            msg = &#39;&lt;WordVectorizer(Vocab Size: {}, Max Words: {})&#39;.format(
                len(self.token2id), self.words_stats[&#39;max&#39;])
        return msg

    def __repr__(self):
        return self.__str__()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="keras_nlp.preprocessing.text.Vectorizer" href="#keras_nlp.preprocessing.text.Vectorizer">Vectorizer</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="keras_nlp.preprocessing.text.SentWordVectorizer" href="#keras_nlp.preprocessing.text.SentWordVectorizer">SentWordVectorizer</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="keras_nlp.preprocessing.text.Vectorizer" href="#keras_nlp.preprocessing.text.Vectorizer">Vectorizer</a></b></code>:
<ul class="hlist">
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.fit_on_texts" href="#keras_nlp.preprocessing.text.Vectorizer.fit_on_texts">fit_on_texts</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.stats" href="#keras_nlp.preprocessing.text.Vectorizer.stats">stats</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.texts_to_vectors" href="#keras_nlp.preprocessing.text.Vectorizer.texts_to_vectors">texts_to_vectors</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.vectors_to_texts" href="#keras_nlp.preprocessing.text.Vectorizer.vectors_to_texts">vectors_to_texts</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="keras_nlp.preprocessing" href="index.html">keras_nlp.preprocessing</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="keras_nlp.preprocessing.text.calc_stats" href="#keras_nlp.preprocessing.text.calc_stats">calc_stats</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="keras_nlp.preprocessing.text.CharVectorizer" href="#keras_nlp.preprocessing.text.CharVectorizer">CharVectorizer</a></code></h4>
</li>
<li>
<h4><code><a title="keras_nlp.preprocessing.text.SentCharVectorizer" href="#keras_nlp.preprocessing.text.SentCharVectorizer">SentCharVectorizer</a></code></h4>
</li>
<li>
<h4><code><a title="keras_nlp.preprocessing.text.SentWordVectorizer" href="#keras_nlp.preprocessing.text.SentWordVectorizer">SentWordVectorizer</a></code></h4>
</li>
<li>
<h4><code><a title="keras_nlp.preprocessing.text.Vectorizer" href="#keras_nlp.preprocessing.text.Vectorizer">Vectorizer</a></code></h4>
<ul class="">
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.fit_on_texts" href="#keras_nlp.preprocessing.text.Vectorizer.fit_on_texts">fit_on_texts</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.stats" href="#keras_nlp.preprocessing.text.Vectorizer.stats">stats</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.texts_to_vectors" href="#keras_nlp.preprocessing.text.Vectorizer.texts_to_vectors">texts_to_vectors</a></code></li>
<li><code><a title="keras_nlp.preprocessing.text.Vectorizer.vectors_to_texts" href="#keras_nlp.preprocessing.text.Vectorizer.vectors_to_texts">vectors_to_texts</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="keras_nlp.preprocessing.text.WordVectorizer" href="#keras_nlp.preprocessing.text.WordVectorizer">WordVectorizer</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>