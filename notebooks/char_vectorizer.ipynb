{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data\n",
    "Download the IMDB reviews if its not already in the local cache.\n",
    "Load from the local cache and split to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example from the reviews:\n",
      "============================\n",
      "Text : If you are expecting to see a lot in the bath of Alt&#305;oklar, as it is promised by the tag line, you will be very disappointed to see that the movie consists of nothing but the populist style of Al...\n",
      "Label: neg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "from keras.utils import get_file\n",
    "try:\n",
    "    from notebooks.data import load_imdb\n",
    "except ModuleNotFoundError:\n",
    "    from data import load_imdb\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '')\n",
    "        and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "zip_file = get_file('aclImdb.zip', origin='http://mng.bz/0tIo', extract=True)\n",
    "imdb_dir = zip_file.replace('.zip', '')\n",
    "(train_texts, train_labels), (test_texts, test_labels) = load_imdb(imdb_dir)\n",
    "\n",
    "print('An example from the reviews:')\n",
    "print('============================')\n",
    "print(f'Text : {train_texts[0][:200]}...')\n",
    "print(f'Label: {train_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare Data\n",
    "Initialize a `CharVectorizer`. The vectorizer will be fitted on the \n",
    "`train_texts`. We won't use any word tokenizer, so the words will be splitted\n",
    "on spaces. The vocabulary will have only the characters set on the \n",
    "`characters` attribute, the PAD and the OOV token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Feb-03 16:49:36 [INFO    :CharVectorizer] - Creating vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 6s 228us/step\n",
      "Vectorizer number of tokens: 28\n"
     ]
    }
   ],
   "source": [
    "from keras_nlp import CharVectorizer\n",
    "\n",
    "char_vectorizer = CharVectorizer(\n",
    "    characters='abcdefghijklmnopqrstuvwxyz', oov_token='#')\n",
    "char_vectorizer.fit_on_texts(train_texts)\n",
    "# The vectorizer's number of tokens: num_chars + PAD + OOV token\n",
    "print(f'Vectorizer number of tokens: {len(char_vectorizer.token2id)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vectorize Data\n",
    "We apply the fitted vectorizer onto the train and test texts. \n",
    "We keep `max_tokens` per text and `max_characters` per token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Feb-03 16:49:43 [INFO    :CharVectorizer] - Converting texts to vectors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 19s 777us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Feb-03 16:50:03 [INFO    :CharVectorizer] - Reshaping vectors to shape (1000, 10).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 14s 545us/step\n"
     ]
    }
   ],
   "source": [
    "max_tokens, max_characters = 1000, 10\n",
    "X_train = char_vectorizer.texts_to_vectors(\n",
    "    train_texts, shape=(max_tokens, max_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Attention**: We *must* pass the same shape `(max_tokens, max_characters)` \n",
    "when converting different text sets. If we don't, then it is almost certain \n",
    "that the results will have different numbers of columns because the sets are \n",
    "likely to have different number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Feb-03 16:50:17 [INFO    :CharVectorizer] - Converting texts to vectors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 17s 675us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Feb-03 16:50:35 [INFO    :CharVectorizer] - Reshaping vectors to shape (1000, 10).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10370/25000 [===========>..................] - ETA: 7s"
     ]
    }
   ],
   "source": [
    "X_test = char_vectorizer.texts_to_vectors(\n",
    "    test_texts, shape=(max_tokens, max_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Decode Data\n",
    "We print a decoded fragment of a encoded text as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "decoded = char_vectorizer.vectors_to_texts(X_test[:1])\n",
    "print('Example fragment of decoded review')\n",
    "print('==================================')\n",
    "print(decoded[0][67:76])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Encode Labels\n",
    "The labels are strings ('pos' / 'neg'). We will convert them to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_train = label_binarize(train_labels, classes=['neg', 'pos'])\n",
    "y_test = label_binarize(test_labels, classes=['neg', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keep a Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, stratify=y_train, shuffle=True, random_state=44)\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_dev  : {X_val.shape}')\n",
    "print(f'X_test : {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Network\n",
    "Here we define a toy network for demonstration purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import (Input, TimeDistributed, Embedding, Flatten, Dense,\n",
    "                          Dropout)\n",
    "\n",
    "chars_input = Input(\n",
    "    shape=(\n",
    "        max_tokens,\n",
    "        max_characters,\n",
    "    ), name='Input', dtype='int32')\n",
    "chars_embeddings = TimeDistributed(\n",
    "    Embedding(\n",
    "        input_dim=char_vectorizer.num_tokens,\n",
    "        output_dim=20,\n",
    "        input_length=max_characters,\n",
    "        mask_zero=False,\n",
    "        trainable=True),\n",
    "    name='Embeddings')(chars_input)\n",
    "x = Dropout(0.4, name='Input_Dropout')(chars_embeddings)\n",
    "x = Flatten(name='Flatten')(x)\n",
    "x = Dropout(0.4, name='Dropout')(x)\n",
    "predictions = Dense(1, activation='sigmoid', name='Predictions')(x)\n",
    "model = Model(chars_input, predictions)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, batch_size=50, epochs=5, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "print(f'Evaluation accuracy: {100*scores[1]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plot Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 6))\n",
    "ax1.plot(history.epoch, history.history['acc'], label='Training')\n",
    "ax1.plot(history.epoch, history.history['val_acc'], label='Validation')\n",
    "ax1.set_title('Accuracy')\n",
    "\n",
    "ax2.plot(history.epoch, history.history['loss'], label='Training')\n",
    "ax2.plot(history.epoch, history.history['val_loss'], label='Validation')\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
