{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data\n",
    "Download the IMDB reviews if its not already in the local cache.\n",
    "Load from the local cache and split to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example from the reviews:\n",
      "============================\n",
      "Text : Entertainment Tonight has been going down hill for the last few years, but as of last night (Aug 17th 2006) they reached a new low.<br /><br />In an effort to try to hype up their broadcast, they deci...\n",
      "Label: neg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "from tensorflow.keras.utils import get_file\n",
    "try:\n",
    "    from notebooks.data import load_imdb\n",
    "except ModuleNotFoundError:\n",
    "    from data import load_imdb\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '')\n",
    "        and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "zip_file = get_file('aclImdb.zip', origin='http://mng.bz/0tIo', extract=True)\n",
    "imdb_dir = zip_file.replace('.zip', '')\n",
    "(train_texts, train_labels), (test_texts, test_labels) = load_imdb(imdb_dir)\n",
    "\n",
    "print('An example from the reviews:')\n",
    "print('============================')\n",
    "print(f'Text : {train_texts[0][:200]}...')\n",
    "print(f'Label: {train_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare Data\n",
    "Initialize a `WordVectorizer`. The vectorizer will be fitted on the \n",
    "`train_texts`. We won't use any word tokenizer, so the words will be splitted\n",
    "on spaces and keep the `num_words` most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Apr-10 00:06:11 [INFO    :WordVectorizer] - Creating vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4s 144us/step\n",
      "Vectorizer number of tokens: 50002\n"
     ]
    }
   ],
   "source": [
    "from keras_nlp import WordVectorizer\n",
    "\n",
    "num_words = 50000\n",
    "word_vectorizer = WordVectorizer(num_words=num_words, oov_token='UNK')\n",
    "word_vectorizer.fit_on_texts(train_texts)\n",
    "# The vectorizer's number of tokens num_words + PAD + OOV token\n",
    "print(f'Vectorizer number of tokens: {word_vectorizer.num_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Encode Labels\n",
    "The labels are strings ('pos' / 'neg'). We will convert them to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_train = label_binarize(train_labels, classes=['neg', 'pos'])\n",
    "y_test = label_binarize(test_labels, classes=['neg', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keep a Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, y_train, y_val = train_test_split(\n",
    "    train_texts, y_train, stratify=train_labels, random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generators for Vectorized Data\n",
    "We use the `texts_to_vectors_generator` method. The generator expects the \n",
    "texts, the labels and other parameters such as `shape` and `batch_size`.\n",
    "For the `shape` we define to be `max_tokens` per sentences.\n",
    "\n",
    "**Attention**: We *must* pass the same number of `max_tokens` when \n",
    "converting different text sets (train, validation, test). \n",
    "If we don't, then it is almost certain that the results will have different \n",
    "numbers of columns because the set are likely to have different number of tokens.\n",
    "We create *generators* for the train, validation and test text sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_tokens = 500\n",
    "batch_size = 32\n",
    "train_generator = word_vectorizer.texts_to_vectors_generator(\n",
    "    train_texts, y_train, shape=(max_tokens,), batch_size=batch_size)\n",
    "val_generator = word_vectorizer.texts_to_vectors_generator(\n",
    "    val_texts, y_val, shape=(max_tokens,), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Network\n",
    "Here we define a toy network for demonstration purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "Embeddings (Embedding)       (None, 500, 100)          5000200   \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "Predictions (Dense)          (None, 1)                 50001     \n",
      "=================================================================\n",
      "Total params: 5,050,201\n",
      "Trainable params: 5,050,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense\n",
    "\n",
    "words_input = Input(shape=(max_tokens, ), name='Input', dtype='int32')\n",
    "words_embeddings = Embedding(\n",
    "    input_dim=len(word_vectorizer.token2id),\n",
    "    output_dim=100,\n",
    "    input_length=max_tokens,\n",
    "    mask_zero=0,\n",
    "    trainable=True,\n",
    "    name='Embeddings')(words_input)\n",
    "x = Flatten(name='Flatten')(words_embeddings)\n",
    "predictions = Dense(1, activation='sigmoid', name='Predictions')(x)\n",
    "model = Model(words_input, predictions)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Apr-10 00:06:15 [WARNING :tensorflow  ] - sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Apr-10 00:06:15 [WARNING :tensorflow  ] - sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 586 steps, validate for 196 steps\n",
      "Epoch 1/5\n",
      "586/586 [==============================] - 38s 64ms/step - loss: 0.4230 - acc: 0.7963 - val_loss: 0.3272 - val_acc: 0.8600\n",
      "Epoch 2/5\n",
      "586/586 [==============================] - 40s 69ms/step - loss: 0.1140 - acc: 0.9644 - val_loss: 0.2685 - val_acc: 0.8923\n",
      "Epoch 3/5\n",
      "586/586 [==============================] - 38s 65ms/step - loss: 0.0271 - acc: 0.9959 - val_loss: 0.2966 - val_acc: 0.8842\n",
      "Epoch 4/5\n",
      "538/586 [==========================>...] - ETA: 2s - loss: 0.0079 - acc: 0.9997"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(train_texts) // batch_size\n",
    "if steps_per_epoch % batch_size:\n",
    "    steps_per_epoch += 1\n",
    "val_steps_per_epoch = len(val_texts) // batch_size\n",
    "if val_steps_per_epoch % batch_size:\n",
    "    val_steps_per_epoch += 1\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x=train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=val_steps_per_epoch,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "steps = len(test_texts) // batch_size\n",
    "if len(test_texts) % batch_size:\n",
    "    steps += 1\n",
    "test_generator = word_vectorizer.texts_to_vectors_generator(\n",
    "    test_texts, y_test, shape=(max_tokens,), batch_size=batch_size)\n",
    "predictions = model.predict(test_generator, steps=steps, verbose=1)\n",
    "y_pred = np.round(predictions)\n",
    "print(classification_report(\n",
    "    y_test, y_pred, digits=4, target_names=['neg', 'pos']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Plot Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 6))\n",
    "ax1.plot(history.epoch, history.history['acc'], label='Training')\n",
    "ax1.plot(history.epoch, history.history['val_acc'], label='Validation')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.epoch, history.history['loss'], label='Training')\n",
    "ax2.plot(history.epoch, history.history['val_loss'], label='Validation')\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
