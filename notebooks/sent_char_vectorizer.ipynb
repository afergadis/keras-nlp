{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data\n",
    "Download the IMDB reviews if its not already in the local cache.\n",
    "Load from the local cache and split to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example from the reviews:\n",
      "============================\n",
      "Text : Entertainment Tonight has been going down hill for the last few years, but as of last night (Aug 17th 2006) they reached a new low.<br /><br />In an effort to try to hype up their broadcast, they deci...\n",
      "Label: neg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "from tensorflow.keras.utils import get_file\n",
    "try:\n",
    "    from notebooks.data import load_imdb\n",
    "except ModuleNotFoundError:\n",
    "    from data import load_imdb\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)): \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    \n",
    "zip_file = get_file('aclImdb.zip', origin='http://mng.bz/0tIo', extract=True)\n",
    "imdb_dir = zip_file.replace('.zip', '')\n",
    "(train_texts, train_labels), (test_texts, test_labels) = load_imdb(imdb_dir)\n",
    "\n",
    "print('An example from the reviews:')\n",
    "print('============================')\n",
    "print(f'Text : {train_texts[0][:200]}...')\n",
    "print(f'Label: {train_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare Data\n",
    "Initialize a `SentCharVectorizer`. The vectorizer will be fitted on the \n",
    "`train_texts`. We will use a set a simple sentence tokenizer and no word \n",
    "tokenizer, so the words will be splitted on spaces. The vocabulary is \n",
    "restricted to the provided characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Apr-09 23:16:07 [INFO    :SentCharVectorizer] - Creating vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 135us/step\n",
      "Vectorizer number of tokens: 28\n"
     ]
    }
   ],
   "source": [
    "from keras_nlp.preprocessing import sent_tokenize\n",
    "from keras_nlp import SentCharVectorizer\n",
    "\n",
    "sent_char_vectorizer = SentCharVectorizer(\n",
    "    sent_tokenize, characters='abcdefghijklmnopqrstuvwxyz', oov_token='#')\n",
    "sent_char_vectorizer.fit_on_texts(train_texts)\n",
    "# The vectorizer's number of tokens num_words + PAD + OOV token\n",
    "print(f'Vectorizer number of tokens: {sent_char_vectorizer.num_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vectorize Data\n",
    "We apply the fitted vectorizer to the train and test texts. \n",
    "Also we have to set the `max_sentences` per text and `max_words` per sentence\n",
    "and `max_characters` per word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Apr-09 23:16:11 [INFO    :SentCharVectorizer] - Converting texts to vectors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 15s 617us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Apr-09 23:16:28 [INFO    :SentCharVectorizer] - Reshaping vectors to shape (10, 15, 30).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 16s 624us/step\n"
     ]
    }
   ],
   "source": [
    "max_sentences, max_tokens, max_characters = 10, 15, 30\n",
    "X_train = sent_char_vectorizer.texts_to_vectors(\n",
    "    train_texts, shape=(max_sentences, max_tokens, max_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Attention**: We *must* pass the same number `max_sentences` per text and \n",
    "`max_words` per sentence and `max_characters` per word. of `max_tokens` when \n",
    "converting different text sets. If we don't, then it is almost certain that \n",
    "the results will have different numbers of columns because the set are likely \n",
    "to have different number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Apr-09 23:16:44 [INFO    :SentCharVectorizer] - Converting texts to vectors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 14s 573us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Apr-09 23:17:00 [INFO    :SentCharVectorizer] - Reshaping vectors to shape (10, 15, 30).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 14s 556us/step\n"
     ]
    }
   ],
   "source": [
    "X_test = sent_char_vectorizer.texts_to_vectors(\n",
    "    test_texts, shape=(max_sentences, max_tokens, max_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Decode Data\n",
    "Print a decoded fragment of an encoded text as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-Apr-09 23:17:14 [INFO    :SentCharVectorizer] - Converting vectors to texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step\n",
      "[['a', 'n'], ['a', 'd', 'u', 'l', 't'], ['m', 'a', 'g'], ['a', 'n', 'd'], ['i'], ['r', 'e', 'c', 'a', 'l', 'l'], ['s', 'e', 'e', 'i', 'n', 'g'], ['p', 'l', 'e', 'n', 't', 'y'], ['o', 'f'], ['m', 'e', 'n'], ['s'], ['f', 'a', 'c', 'e', 's'], ['w', 'h', 'e', 'n'], ['t', 'h', 'e', 'y'], ['c', 'a', 'm', 'e']]\n"
     ]
    }
   ],
   "source": [
    "decoded = sent_char_vectorizer.vectors_to_texts(X_test[:1])\n",
    "print(decoded[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Encode Labels\n",
    "The labels are strings ('pos' / 'neg'). We will convert them to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_train = label_binarize(train_labels, classes=['neg', 'pos'])\n",
    "y_test = label_binarize(test_labels, classes=['neg', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keep a Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (18750, 10, 15, 30)\n",
      "X_dev  : (6250, 10, 15, 30)\n",
      "X_test : (25000, 10, 15, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, stratify=y_train, shuffle=True, random_state=44)\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_dev  : {X_val.shape}')\n",
    "print(f'X_test : {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Network\n",
    "Here we define a toy network for demonstration purpose.\n",
    "The model has two layers. The first encodes the characters of the words of a \n",
    "sentence providing sentence vectors. The second encodes the sentence\n",
    "vectors to a document vector.\n",
    "\n",
    "## Characters Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 15, 30)]          0         \n",
      "_________________________________________________________________\n",
      "Embeddings (TimeDistributed) (None, 15, 30, 20)        560       \n",
      "_________________________________________________________________\n",
      "Input_Dropout (Dropout)      (None, 15, 30, 20)        0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 9000)              0         \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 9000)              0         \n",
      "=================================================================\n",
      "Total params: 560\n",
      "Trainable params: 560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding, TimeDistributed, \\\n",
    "    Dropout, Flatten, Dense, Bidirectional, LSTM\n",
    "\n",
    "rnn_params = dict(dropout=0.2, return_sequences=True, recurrent_dropout=0.2)\n",
    "\n",
    "chars_input = Input(\n",
    "    shape=(\n",
    "        max_tokens,\n",
    "        max_characters,\n",
    "    ), name='Input', dtype='int32')\n",
    "chars_embeddings = TimeDistributed(\n",
    "    Embedding(\n",
    "        input_dim=sent_char_vectorizer.num_tokens,\n",
    "        output_dim=20,\n",
    "        input_length=max_characters,\n",
    "        mask_zero=False,\n",
    "        trainable=True),\n",
    "    name='Embeddings')(chars_input)\n",
    "x = Dropout(0.4, name='Input_Dropout')(chars_embeddings)\n",
    "x = Flatten(name='Flatten')(x)\n",
    "x = Dropout(0.4, name='Dropout')(x)\n",
    "char_model = Model(chars_input, x)\n",
    "\n",
    "print(char_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________\n",
      "Layer (type)                            Output Shape                        Param #       \n",
      "==========================================================================================\n",
      "Sent_Input (InputLayer)                 [(None, 10, 15, 30)]                0             \n",
      "__________________________________________________________________________________________\n",
      "Char_Model (TimeDistributed)            (None, 10, 9000)                    560           \n",
      "__________________________________________________________________________________________\n",
      "Sents_Encoder (Bidirectional)           (None, 10, 200)                     7280800       \n",
      "__________________________________________________________________________________________\n",
      "Dropout (Dropout)                       (None, 10, 200)                     0             \n",
      "__________________________________________________________________________________________\n",
      "Flatten (Flatten)                       (None, 2000)                        0             \n",
      "__________________________________________________________________________________________\n",
      "Predictions (Dense)                     (None, 1)                           2001          \n",
      "==========================================================================================\n",
      "Total params: 7,283,361\n",
      "Trainable params: 7,283,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sent_input = Input(\n",
    "    shape=(max_sentences, max_tokens, max_characters), dtype='int32', name='Sent_Input')\n",
    "sent_input_over_word_model = TimeDistributed(\n",
    "    char_model, name='Char_Model')(sent_input)\n",
    "sent_encoder = Bidirectional(\n",
    "    LSTM(100, **rnn_params), name='Sents_Encoder')(sent_input_over_word_model)\n",
    "x = Dropout(0.3, name='Dropout')(sent_encoder)\n",
    "x = Flatten(name='Flatten')(x)\n",
    "predictions = Dense(1, activation='sigmoid', name='Predictions')(x)\n",
    "model = Model(sent_input, predictions)\n",
    "\n",
    "print(model.summary(90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/5\n",
      "18750/18750 [==============================] - 84s 4ms/sample - loss: 0.6914 - acc: 0.5463 - val_loss: 0.6575 - val_acc: 0.6096\n",
      "Epoch 2/5\n",
      "18750/18750 [==============================] - 79s 4ms/sample - loss: 0.6411 - acc: 0.6328 - val_loss: 0.6273 - val_acc: 0.6424\n",
      "Epoch 3/5\n",
      "18750/18750 [==============================] - 82s 4ms/sample - loss: 0.6104 - acc: 0.6646 - val_loss: 0.6213 - val_acc: 0.6595\n",
      "Epoch 4/5\n",
      "18750/18750 [==============================] - 82s 4ms/sample - loss: 0.5880 - acc: 0.6825 - val_loss: 0.6178 - val_acc: 0.6558\n",
      "Epoch 5/5\n",
      "16500/18750 [=========================>....] - ETA: 8s - loss: 0.5640 - acc: 0.7065"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train, \n",
    "                    batch_size=50, \n",
    "                    epochs=5, \n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "print(f'Evaluation accuracy: {100*scores[1]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Plot Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 6))\n",
    "ax1.plot(history.epoch, history.history['acc'], label='Training')\n",
    "ax1.plot(history.epoch, history.history['val_acc'], label='Validation')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.epoch, history.history['loss'], label='Training')\n",
    "ax2.plot(history.epoch, history.history['val_loss'], label='Validation')\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
